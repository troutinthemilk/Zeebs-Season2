---
title: "Ratio estimates of density"
name: "St Clair"
date: "`r format(Sys.Date())`"
output: 
  pdf_document
---


```{r setup, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = TRUE, comment = NULL, collapse = TRUE, prompt = TRUE, warning = FALSE, message = FALSE, include = FALSE)

library(readxl)
library(survey)
library(tidyverse)
library(unmarked)
library(mrds)
library(knitr)
ests.df <- data_frame(Lake = rep(NA, 3*5), Dhat = rep(NA, 3*5), SE = rep(NA, 3*5), method = rep(NA,3*5), detProb = rep(NA, 3*5)) 
```




```{r}
double.dat <- read_xlsx(path="../Data/Season2/Encounters - Quadrats (Responses).xlsx", sheet=1) %>% select(-1)
double.dat %>% group_by(`Lake name`, `Observer name`, `Transect #`, `Distance along transect (m)`) %>% count() %>% filter(n > 1) %>% print(width = Inf)
```
```{r, eval = FALSE, include=FALSE}
# one duplicated entry (Aislyn, Burgan, tran3, dist2)
double.dat <- distinct(double.dat)
double.dat %>% group_by(`Lake name`,  `Transect #`) %>% count() %>% summarize(odd = ((n %% 2) == 1) )%>% filter(odd)
double.dat %>% filter(`Lake name` == "Lake Burgan", `Transect #` ==11)
# row 144 should be tran 15 and not 12
#double.dat$`Transect #`[144] <- 15
```

#### Quad sampling: assuming perfect detection
For each transect $i$ ($i = 1, \dotsc, T$), we observe mussel counts in $n_i$ quadrats. The total number of mussels for each transect is $y_i$ (summed over all quads) and the total area sampled in a transect is $a_i = n_i \times 0.5^2$ square-meters. The total area and number of quads sampled varies across quads, so we will use a ratio estimate of density:
$$\hat{D} = \dfrac{\sum_{i=1}^T y_i}{\sum_{i=1}^T a_i}$$

We can use the `survey` package to obtain this ratio estimate and SE for each lake. The design assumes a SRS of transects around the lake with mussel counts and area recorded fro each transect. 
```{r}
double.tran <- double.dat %>% 
  group_by(`Transect #`, `Lake name`) %>%
  summarize(y = sum(`Number of mussels in quadrat`), ni = n(), area = ni*.5*.5)
double.tran %>% group_by(`Lake name`) %>% summarize(totalY = sum(y), totalArea = sum(area))
des <- svydesign(data = double.tran, id = ~1, strata = ~`Lake name`)
svy.out <- svyratio(~y, ~area, des, separate = TRUE)
svy.out
ests.df$Lake[1:3] <- svy.out$strata
ests.df$method[1:3] <- "quads, ratio"
ests.df$Dhat[1:3] <- sapply(svy.out$strata, function(x) svy.out[[1]][[x]][[1]])
ests.df$SE[1:3] <- sqrt(sapply(svy.out$strata, function(x) svy.out[[1]][[x]][[2]]))
ests.df$detProb[(1:3)] <- c(1,1,1) 
```

#### Transect sampling (no distance): assuming perfect detection
For each transect $i$ ($i = 1, \dotsc, T$), let $l_i$ be the length of the transect sampled. The total number of mussels for each transect observed by **both** divers is $y_i$. The total area sampled in a transect is $a_i = l_i \times w \times 2$ square-meters where $w$ is the (half-width) distance from the transect that divers looked for mussels. The total area and number of quads sampled varies across quads, so we will use a ratio estimate of density:
$$\hat{D} = \dfrac{\sum_{i=1}^T y_i}{\sum_{i=1}^T a_i}$$

Results for transects 1-8, assuming $w = 0.5$m:

```{r}
double.dat      <- read_xlsx(path="../Data/Season2/Encounters - Double observer - no distance (Responses).xlsx", sheet=1) %>% select(-1)
double.dat$`Number of mussels in cluster`[is.na(double.dat$`Number of mussels in cluster`)] <- 0
```
```{r}
transect.dat      <- read_xlsx(path="../Data/Season2/Transect datasheets (Responses).xlsx", sheet=1) %>% 
  filter(`Survey type` == "Double observer no distance") %>%
  select(`Transect number`, `Transect length (if transect survey)`, `Lake name:`)
dim(transect.dat)
transect.dat <- transect.dat %>% dplyr::rename(`Transect #` = `Transect number`,length = `Transect length (if transect survey)`) %>%
  mutate(`Lake name` = recode(`Lake name:`, Burgan = "Lake Burgan", Florida = "Lake Florida", `Little Birch Lake` = "Little Birch Lake"))
```
```{r}
double.tran <- double.dat %>% 
  group_by(`Transect #`, `Lake name`) %>%
  summarize(y = sum(`Number of mussels in cluster`))
dim(double.tran)
double.tran <- left_join(double.tran, transect.dat, by = c("Lake name", "Transect #"))
double.tran %>% group_by(`Lake name`) %>% summarize(totalY = sum(y), totalArea = sum(length))
des <- svydesign(data = double.tran, id = ~1, strata = ~`Lake name`)
svy.out <- svyratio(~y, ~length, des, separate = TRUE)
svy.out
ests.df$Lake[(1:3)+3] <- svy.out$strata
ests.df$method[(1:3)+3] <- "Double no distance, ratio"
ests.df$Dhat[(1:3)+3] <- sapply(svy.out$strata, function(x) svy.out[[1]][[x]][[1]])
ests.df$SE[(1:3)+3] <- sqrt(sapply(svy.out$strata, function(x) svy.out[[1]][[x]][[2]]))
ests.df$detProb[(1:3)+3] <- c(1,1,1) 
```




### Try using `unmarked` for double observer removal data
This model uses multinomial distribution for observed removal counts and a Poisson model for transect population mussels counts, denoted $N_i$. Let $y_{ij}$ be the number of mussels (not clusters) removed at time $j$ at transect $i$, and $\pmb{y_i} = (y_{i1}, y_{i2}, y_{i0})$ where $y_{i0} = N_i - \sum_{j}y_{ij}$ is the number of unobserved mussels. Then the model is
$$N_i \sim Pois(\lambda_i) \ \ \\
\pmb{y_i} \mid N_i \sim Multinom(N_i, \pmb{\pi})$$
Since transects vary in area, we allow mean abundance $\lambda_i$ to depend on the area of the site using the model
$$\log(\lambda_i) = \beta_0 + \beta_1 a_i$$
This model is fit for Burgan and Little Birch since transect lengths varied. Area is not used in the model for Florida since all transects were 30m in length. 

Estimated density is formed from transect level estimates of abundance $\hat{N}_i$:
$$\hat{D} = \dfrac{\sum_{i}\hat{N}_i}{A}$$



To look at: 

- $\beta_1$ estimates are negative?? When excluding intercept it is positive and the $\hat{D}$ estimates are slightly lower for Burgan (LBL is about the same)
- use negative binomial model for $N$

```{r}
transect.dat      <- read_xlsx(path="../Data/Season2/Transect datasheets (Responses).xlsx", sheet=1) %>% 
  filter(`Survey type` == "Double observer no distance") %>%
  select(`Transect number`, `Transect length (if transect survey)`, `Lake name:`, `Primary observer (double observer survey)`, `Secondary observer (double observer survey)`)
transect.dat <- transect.dat %>% dplyr::rename(`Transect #` = `Transect number`,length = `Transect length (if transect survey)`, primary = `Primary observer (double observer survey)`, secondary = `Secondary observer (double observer survey)`) %>%
  mutate(`Lake name` = recode(`Lake name:`, Burgan = "Lake Burgan", Florida = "Lake Florida", `Little Birch Lake` = "Little Birch Lake"))
double.dat      <- read_xlsx(path="../Data/Season2/Encounters - Double observer - no distance (Responses).xlsx", sheet=1) %>% select(-1)
double.dat$`Number of mussels in cluster`[is.na(double.dat$`Number of mussels in cluster`)] <- 0
```
```{r}
#restructure transect data
transect.dat <- transect.dat %>% gather(key = observer, value = name, "primary", "secondary") 
# join to add primary or secondary and length measures to double counts
double.dat <- left_join(double.dat, transect.dat, by = c("Lake name", "Transect #", "Observer name"= "name")) 

counts <- double.dat %>% 
  group_by(`Lake name`, `Transect #`, observer) %>% 
  summarize(y = sum(`Number of mussels in cluster`))
counts %>% group_by(`Lake name`, observer) %>%
  summarize(meany = mean(y), medy = median(y), toty = sum(y), n = n())
counts.wide <- counts %>% 
  spread(key = observer, value = y)

left_join(counts.wide, select(double.dat,`Lake name`, `Transect #`, length), by=c("Lake name", "Transect #")) %>%
  ggplot(aes(x=length, y = primary + secondary)) + geom_point() + facet_wrap(~`Lake name`) + scale_x_log10()

# unmarked package
# get  removal count matrices and covariate (area) transect data frames
count.mat.b <- as.matrix(counts.wide[counts.wide$`Lake name` == "Lake Burgan",3:4])
covDf.b <- double.dat %>% filter(`Lake name` == "Lake Burgan") %>%
  group_by(`Transect #`) %>%
  summarize(area = first(length) )
count.mat.f <- as.matrix(counts.wide[counts.wide$`Lake name` == "Lake Florida",3:4])
covDf.f <- double.dat %>% filter(`Lake name` == "Lake Florida") %>%
  group_by(`Transect #`) %>%
  summarize(area = first(length) )
count.mat.lb <- as.matrix(counts.wide[counts.wide$`Lake name` == "Little Birch Lake",3:4])
covDf.lb <- double.dat %>% filter(`Lake name` == "Little Birch Lake") %>%
  group_by(`Transect #`) %>%
  summarize(area = first(length) )
```


```{r}
# fit models, using area as covariate (though coeff for area is negative???)
bFrame <- unmarkedFrameMPois(y=count.mat.b, siteCovs = covDf.b, type="removal")
fFrame <- unmarkedFrameMPois(y=count.mat.f, siteCovs = covDf.f, type="removal")
lbFrame <- unmarkedFrameMPois(y=count.mat.lb, siteCovs = covDf.lb, type="removal")
bout <- multinomPois(~ area ~ area , data = bFrame)  # area better AIC than log(area)
bout
bout.o <- multinomPois(~1 ~ offset(log(area)) , data = bFrame)
bout.o
bout@estimates
fout <- multinomPois(~1 ~ 1 , data = fFrame)  # lengths same for all Florida plots
fout
lbout <- multinomPois(~area ~ area, data = lbFrame)  # log area produces NAs
lbout
lbout.o <- multinomPois(~1 ~ offset(log(area)), data = lbFrame)
lbout.o

# using area as a covariate is better than an offset.
# using area for both abundance and detection looks best for AIC and GOF tests
```

```{r}
abund.b<- backTransform(linearComb(bout, type = "state", coefficients = matrix(c(rep(1,nrow(covDf.b)), covDf.b$area), byrow=FALSE, nrow=nrow(covDf.b)) ))   # intercept
#abund.b<- backTransform(linearComb(bout, type = "state", coefficients = matrix(rep(1,nrow(covDf.b)), ncol=1 ), offset = covDf.b$offset))  # offset
abund.b
# add transect estimates
abund.ball <- sum(abund.b@estimate)
abund.ball
abund.ball/sum(covDf.b$area)
abund.ball.se <- sqrt(sum(vcov(abund.b)))
abund.ball.se
abund.ball.se/sum(covDf.b$area)

abund.f<- backTransform(linearComb(fout, type = "state", coefficients = matrix(c(rep(1,nrow(covDf.f))), byrow=FALSE, nrow=nrow(covDf.f)) ))
abund.f
abund.f30m<- backTransform(linearComb(fout, type = "state", coefficients = c(1) ))
abund.f30m
abund.f30m@estimate/30   # area of florida transects
SE(abund.f30m)/30
# add transect estimates
abund.fall <- sum(abund.f@estimate)
abund.fall
abund.fall/sum(covDf.f$area)
abund.fall.se <- sqrt(sum(vcov(abund.f)))
abund.fall.se
abund.fall.se/sum(covDf.f$area)


abund.lb<- backTransform(linearComb(lbout, type = "state", coefficients = matrix(c(rep(1,3), covDf.lb$area), byrow=FALSE, nrow=nrow(covDf.lb)) )) # intercept
#abund.lb<- backTransform(linearComb(bout, type = "state", coefficients = matrix(rep(1,nrow(covDf.lb)), ncol=1 ), offset = covDf.lb$offset))  # offset
abund.lb
# add transect estimates
abund.lball <- sum(abund.lb@estimate)
abund.lball
abund.lball/sum(covDf.lb$area)
abund.lball.se <- sqrt(sum(vcov(abund.lb)))
abund.lball.se
abund.lball.se/sum(covDf.lb$area)
```

```{r}
svy.out$strata
ests.df$Lake[(1:3)+3*2] <- svy.out$strata
ests.df$method[(1:3)+3*2] <- "Double no distance, multiPois"
ests.df$Dhat[(1:3)+3*2] <- c(abund.ball/sum(covDf.b$area), abund.fall/sum(covDf.f$area), abund.lball/sum(covDf.lb$area))
ests.df$SE[(1:3)+3*2] <- c(abund.ball.se/sum(covDf.b$area), abund.fall.se/sum(covDf.f$area), abund.lball.se/sum(covDf.lb$area))
ests.df$detProb[(1:3)+3*2] <- c(
                     mean(backTransform(linearComb(bout, type = "det", coefficients = matrix(c(rep(1,3), covDf.b$area), byrow=FALSE, nrow=nrow(covDf.b))))@estimate),
                     backTransform(linearComb(fout, type = "det", coefficients = 1))@estimate, 
                     mean(backTransform(linearComb(lbout, type = "det", coefficients = matrix(c(rep(1,3), covDf.lb$area), byrow=FALSE, nrow=nrow(covDf.lb))))@estimate))
```


GOF assessment:

```{r gof poission, eval=FALSE}
chisq <- function(x){
  observed <- getY(x@data)
  expected <- fitted(x)
  sum((observed - expected)^2/expected)
}
pb <- parboot(bout, statistic = chisq, nsim = 500)
plot(pb)
pb <- parboot(fout, statistic = chisq, nsim = 500)
plot(pb)
pb <- parboot(lbout, statistic = chisq, nsim = 500)
plot(pb)
```

#### Using neg binom abundance model

```{r}
# fit models, using area as covariate (though coeff for area is negative???)
bFrame <- unmarkedFrameGMM(y=count.mat.b, siteCovs = covDf.b, type="removal", numPrimary = 1)
fFrame <- unmarkedFrameGMM(y=count.mat.f, siteCovs = covDf.f, type="removal", numPrimary = 1)
lbFrame <- unmarkedFrameGMM(y=count.mat.lb, siteCovs = covDf.lb, type="removal", numPrimary = 1)
bout <- gmultmix(~ area, ~1, ~ area , data = bFrame, mixture = "NB")  # area better AIC than log(area)
bout
#backTransform(bout, type="lambda")
fout <- gmultmix(~1, ~ 1 , ~1, data = fFrame, mixture = "NB")  # lengths same for all Florida plots
fout
# lbout <- gmultmix(~log(area), ~1, ~ log(area), data = lbFrame, mixture = "NB", starts = c(3,.1,-20,.5,1))  # log area produces NAs
# lbout
# 
lbout <- gmultmix(~1, ~1, ~ 1, data = lbFrame, mixture = "P") #, control = list(trace=2), starts = c(30.1399659,   .1026389, 5.1867414,   2.650632,5), K=400)  # log area produces NAs
lbout
```

GOF assessment:
```{r gof nb, eval = FALSE}
pb <- parboot(bout, statistic = chisq, nsim = 500)
pb
plot(pb)
pb <- parboot(fout, statistic = chisq, nsim = 500)
plot(pb)
pb
# pb <- parboot(lbout, statistic = chisq, nsim = 500)
# plot(pb)
# pb
```

```{r}
abund.b<- backTransform(linearComb(bout, type = "lambda", coefficients = matrix(c(rep(1,nrow(covDf.b)), covDf.b$area), byrow=FALSE, nrow=nrow(covDf.b)) ))   # intercept
#abund.b<- backTransform(linearComb(bout, type = "state", coefficients = matrix(rep(1,nrow(covDf.b)), ncol=1 ), offset = covDf.b$offset))  # offset
abund.b
# add transect estimates
abund.ball <- sum(abund.b@estimate)
abund.ball
abund.ball/sum(covDf.b$area)
abund.ball.se <- sqrt(sum(vcov(abund.b)))
abund.ball.se
abund.ball.se/sum(covDf.b$area)

abund.f<- backTransform(linearComb(fout, type = "lambda", coefficients = matrix(c(rep(1,nrow(covDf.f))), byrow=FALSE, nrow=nrow(covDf.f)) ))
abund.f
abund.f30m<- backTransform(linearComb(fout, type = "lambda", coefficients = c(1) ))
abund.f30m
abund.f30m@estimate/30   # area of florida transects
SE(abund.f30m)/30
# add transect estimates
abund.fall <- sum(abund.f@estimate)
abund.fall
abund.fall/sum(covDf.f$area)
abund.fall.se <- sqrt(sum(vcov(abund.f)))
abund.fall.se
abund.fall.se/sum(covDf.f$area)


# abund.lb<- backTransform(linearComb(lbout, type = "state", coefficients = matrix(c(rep(1,3), covDf.lb$area), byrow=FALSE, nrow=nrow(covDf.lb)) )) # intercept
# #abund.lb<- backTransform(linearComb(bout, type = "state", coefficients = matrix(rep(1,nrow(covDf.lb)), ncol=1 ), offset = covDf.lb$offset))  # offset
# abund.lb
# # add transect estimates
# abund.lball <- sum(abund.lb@estimate)
# abund.lball
# abund.lball/sum(covDf.lb$area)
# abund.lball.se <- sqrt(sum(vcov(abund.lb)))
# abund.lball.se
# abund.lball.se/sum(covDf.lb$area)
```

```{r}
svy.out$strata
ests.df$Lake[(1:3)+3*4] <- svy.out$strata
ests.df$method[(1:3)+3*4] <- "Double no distance, multiNegBinom"
ests.df$Dhat[(1:3)+3*4] <- c(abund.ball/sum(covDf.b$area), abund.fall/sum(covDf.f$area),NA) # abund.lball/sum(covDf.lb$area))
ests.df$SE[(1:3)+3*4] <- c(abund.ball.se/sum(covDf.b$area), abund.fall.se/sum(covDf.f$area), NA) # abund.lball.se/sum(covDf.lb$area))
ests.df$detProb[(1:3)+3*4] <- c(
                     mean(backTransform(linearComb(bout, type = "det", coefficients = matrix(c(rep(1,3), covDf.b$area), byrow=FALSE, nrow=nrow(covDf.b))))@estimate),
                     backTransform(linearComb(fout, type = "det", coefficients = 1))@estimate, NA)
                     #mean(backTransform(linearComb(lbout, type = "det", coefficients = matrix(c(rep(1,3), covDf.lb$area), byrow=FALSE, nrow=nrow(covDf.lb))))@estimate))
```

#### from ddf removal sampling: assumes $w = 0.5$?? 
Since area is equal to length, assuming that half width is 0.5m. Use the `mrds` package to estimate density as 
$$\hat{D} = \dfrac{n \bar{s}}{\hat{P}_dA}$$
where $\bar{s}$ is the mean mussel count per detection, $\hat{P}_d$ is the estimated probability of detection on a transect (proportion of actual clusters that were detected), and $A$ is the total area surveyed. 

```{r}
##funciton to create double observer transect data.frame that can be used in ddf.
create.removal.Observer <- function(transect.dat, obs.dat) {
  
  primary   <- transect.dat$`Primary observer (double observer survey)`
  secondary <- transect.dat$`Secondary observer (double observer survey)`
  
  obs.dat <- obs.dat %>% mutate(primary=primary[obs.dat$`Transect #`- min(obs.dat$`Transect #`)+1], secondary=secondary[obs.dat$`Transect #`- min(obs.dat$`Transect #`)+1], observer = rep(1, dim(obs.dat)[1]))
  
  for(i in 1:dim(obs.dat)[1]) {
    curr.tran             <- obs.dat[i,]$`Transect #`
    #obs.dat[i,]$observer  <- as.numeric(obs.dat[i,]$`Observer name` == transect.dat[transect.dat$`Transect number` == curr.tran,]$`Primary observer (double observer survey)`)
    obs.dat[i,]$observer  <- as.numeric(obs.dat[i,]$`Observer name` == transect.dat[transect.dat$`Transect number` == curr.tran,]$`Primary observer (double observer survey)`)
  }

  
  #all observations made by secondary that were not made by primary
  temp01.obs <- obs.dat[obs.dat$observer==0,]
  if(dim(temp01.obs)[1]) {
    temp01.obs$observer <- 1
    temp01.obs$detected <- 0
  }
  #print(dim(temp01.obs)[1])
  temp012.obs <- temp01.obs
  if(dim(temp012.obs)[1]) {
    temp012.obs$observer <- 0
    temp012.obs$detected <- 1
  }
  
  #obs.dat <- rbind(obs.dat, temp.obs)
  
  #copy all observations made by the primary observer to the secondary observer
  temp11.obs <- obs.dat[obs.dat$observer==1,]
  temp11.obs$observer <- 0
  temp11.obs$detected <- 1
  temp112.obs <- temp11.obs
  temp112.obs$observer <- 1
  temp112.obs$detected <- 1
  #obs.dat <- rbind(obs.dat, temp.obs)
  
  obs.dat <- rbind(temp01.obs, temp012.obs, temp11.obs, temp112.obs)
  
  obs.dat$observer[which(obs.dat$observer==0)] <- 2
  
  obs.dat <- obs.dat[order(obs.dat$object),]
  
  return(obs.dat)
}
```
```{r}
double.dat.orig      <- read_xlsx(path="../Data/Season2/Encounters - Double observer - no distance (Responses).xlsx", sheet=1)
transect.dat.orig      <- read_xlsx(path="../Data/Season2/Transect datasheets (Responses).xlsx", sheet=1)
curr.lake <- "Lake Burgan"
#curr.lake <- "Lake Florida"
#curr.lake <- "Little Birch Lake"
curr.lake2 <- "Burgan"
#curr.lake2 <- "Florida"
#curr.lake2 <- "Little Birch Lake"
```

```{r Jake ests}
  double.dat      <- double.dat.orig %>% subset(`Lake name` == curr.lake)
  double.dat      <- double.dat[order(double.dat$`Transect #`),]
  double.dat      <- dplyr::rename(double.dat, size="Number of mussels in cluster")
  doubleorig.dat <- double.dat


  if(any(double.dat$size==0)) {
    double.dat <- double.dat[-which(double.dat$size==0),]
  }
  if(any(is.na(double.dat$size))) {
    double.dat <- double.dat[-which(is.na(double.dat$size)),]
  }
  
(  eS <- mean(double.dat$size, na.rm=T) )
(  varS <- var(double.dat$size, na.rm=T)/length(double.dat$size) )


  transect.dat <- transect.dat.orig %>% subset(`Lake name:` == curr.lake2)
  transect.dat <- transect.dat %>% subset(`Survey type` == "Double observer no distance")
  transect.dat <- transect.dat[order(transect.dat$"Transect number"),]
 
  trans.length  <- transect.dat$`Transect length (if transect survey)`
  trans.area    <- trans.length

  #get number of obsevations made by primary and secondary observers in each transect
  count.primary <- count.secondary <- detect.primary <- detect.secondary <- rep(NA, max(transect.dat$`Transect number`))
  for(i in unique(transect.dat$`Transect number`)) {
    curr.prim <- transect.dat[transect.dat$`Transect number` == i,]$`Primary observer (double observer survey)`
    curr.dat  <- double.dat[double.dat$`Transect #`== i,] 
    
    count.primary[i]    <- max(sum(curr.dat[curr.dat$`Observer name` == curr.prim,]$size), 0, na.rm=T)
    count.secondary[i]  <- max(sum(curr.dat[curr.dat$`Observer name` != curr.prim,]$size), 0, na.rm=T)
    
    detect.primary[i]    <- max(dim(curr.dat[curr.dat$`Observer name` == curr.prim,])[1], 0, na.rm=T)
    detect.secondary[i]  <- max(dim(curr.dat[curr.dat$`Observer name` != curr.prim,])[1], 0, na.rm=T)
    
  }
  if(any(is.na(count.primary))) {
    rm.vec          <- which(is.na(count.primary))
    count.primary   <- count.primary[-rm.vec]
    count.secondary <- count.secondary[-rm.vec]
    detect.primary  <- detect.primary[-rm.vec]
    detect.secondary <- detect.secondary[-rm.vec]
  }
    
  double.dat <- double.dat %>% mutate(detected = rep(1, dim(double.dat)[1]), distance=rep(0, dim(double.dat)[1]), object=1:dim(double.dat)[1])
  
  double.dat$object <- 1:dim(double.dat)[1]
  double.dat <- create.removal.Observer(transect.dat=transect.dat, obs.dat=double.dat)
  
  doubleDetect.model <- ddf(method="rem.fi", mrmodel=~glm(formula=~1), data=double.dat, meta.data=list(width=100))
  phat    <- summary(doubleDetect.model)$average.p0.1
  phat.se <- summary(doubleDetect.model)$average.p.se[1,1]
  
  count.vec <- count.primary + count.secondary
  detect.vec <- detect.primary + detect.secondary
  
  count.vec <- t(as.matrix(count.vec))
  dimnames(count.vec)[[2]] <- transect.dat$`Transect number`
 
( dhat.double   <- sum(detect.vec, na.rm=T)*eS/phat/trans.area )
  #? sum over area?
( dhat.double   <- sum(detect.vec, na.rm=T)*eS/phat/sum(trans.area) )

#( se.double     <- sqrt(dhat.double^2*(sum(double.predict$se.fit^2)/sum(double.predict$fit)^2  + phat.se^2/phat^2))  #+ varS/eS^2 
```
```{r}
ests.df$Lake[10] <- curr.lake
ests.df$method[10] <- "Double no distance, MRDS"
ests.df$Dhat[10] <- dhat.double 
ests.df$detProb[10] <- phat
```

```{r}
curr.lake <- "Lake Florida"
curr.lake2 <- "Florida"
```
```{r Jake ests}
```
```{r}
ests.df$Lake[11] <- curr.lake
ests.df$method[11] <- "Double no distance, MRDS"
ests.df$Dhat[11] <- dhat.double 
ests.df$detProb[11] <- phat
```


```{r}
curr.lake <- "Little Birch Lake"
curr.lake2 <- "Little Birch Lake"
```
```{r Jake ests}
```
```{r}
ests.df$Lake[12] <- curr.lake
ests.df$method[12] <- "Double no distance, MRDS"
ests.df$Dhat[12] <- dhat.double 
ests.df$detProb[12] <- phat
```

#### Results
Note: detection prob rate for `multiPois` is at the mussel level while it is at the cluster level for `MRDS` models.

```{r, include = TRUE}
kable(arrange(ests.df, Lake), digits = 3)
```




