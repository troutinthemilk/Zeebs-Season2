---
title: Tradeoffs in detections efficiency and area covered drive the design of optimal density surveys
author: Jake M. Ferguson, Aislyn Keyes, Michael McCartney, Katie
  St. Clair, Douglas Johnson, John Fieberg
header-includes:
   - \usepackage{amsmath}
   - \usepackage{graphicx}
   - \usepackage{lineno}
   - \linenumbers
   - \usepackage{setspace}\doublespacing
output:
  pdf_document: 
    fig_caption: yes
  html_document: default
bibliography: Zebra_mussels.bib
geometry: margin=1in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library, echo=F, include=FALSE, warning=F}
library(MASS)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(kableExtra)
library(RColorBrewer)
library(wesanderson)
library(grid)
library(plotrix)

load('DensEst.Rdata')

```

# Introduction

* impacts and costs of surveying invasive species: Past work has focused primarily on the early detection [e.g., @Ferguson2014b, @Holden2016] and eradication [@Regan2006] of invasive species or detecting rare and endangered species [@Chades2008], however the complementary problem of designing efficient surveys to determine population distribution and density of animals post-detection is often a larger component of the management efforts. Determining how to optimize these efforts has received much less attention.
* Something about usefulness of Unionids as a study system for studying detection methods [Green1993, Smith2006]...
* There likely exists an empirical tradeoff between survey efficiency and survey coverage in most study designs. Surveys performed deliberately (*word choice*) will likely be time-consuming and thus cover a limited area, but have a higher probability of detecting indidivuals. If a survey is performed quickly it can cover more area at the expense of lower detection rates. 
* Here we explored two types of surveys for zebra mussels. We used transect surveys with distance sampling, which covers a larger area but has imperfect detection, and quadrat surveys, which represent a low efficiency type of survey with high detection probability. We examined how each design performed under a range of densities. Finally, we used the empirical studies to parameterize a general model that allowed us to examine the determine the best survey approach for a given problem based on density.  

# Methods

## Surveys

We conducted two different types of surveys in three Minnesota lakes in 2018. We decided lakes to survey based on initial visits to six different lakes throughout central Minnesota (Christmas Lake, East Lake Sylvia, Lake Burgan, Lake Florida, Little Birch Lake, Sylvia Lake) that have had confirmed recent zebra mussel infestations (as determined by Minnesota Department of Natural Resources). At each lake we visited 15 different sites distributed evenly around the lake. Each site was placed in 3 to 8 m of water and determined using a bathymetry shapefile in ArcMap. We located each point in the field using a GPS unit (Garmin GPSMAP 64s). At each site our team of two divers spent 15 minutes underwater counting zebra mussels. We used these counts to determine the three lakes to conduct full surveys on, selecting lakes that displayed a range of apparent densities. 

Based on our initial 15 minute exploratory dives we conducted full surveys on Lake Florida in Kandiyohi County, Lake Burgan in Douglas County, and Little Birch Lake in Todd County. Lake Florida covers an area of 273 hectares and has a maximum depth of 12 m, Lake Burgan covers an area of 74 hectares and has a maximum depth of 13 m, Little Birch Lake covers 339 hectares and has a maximum depth of 27 m. We surveyed each of the 15 previously selected sites in each lake using two type of surveys; quadrat and distance sampling with removal. 

```{r mapFig, echo=F, fig.asp=1, fig.cap="Map of survey sites, each point indicates the starting location of a transect. Remove place name labels. Fix x-axis on Florida, fix sites on Birch.", warning=F, messages=F}

load("ZeebMaps.Rdata")
x <- grid.arrange(p1, p2, p3, nrow=1, ncol=3)

```

### Quadrat surveys

At each site we used the previously defined transect locations to determine the start of a transect. We ran out parallel 30 m transect lines 1 meter apart perpendicular to the shoreline, though transects were stopped earlier than 30 m if divers ran into the thermocline. Our team of two divers each took one of the transects, placing a $0.5 \times 0.5$ square meter quadrat every 2 meters along the transect. In each quadrat the diver counted all the mussels within the quadrat.  


### Distance removal surveys

At each site we used the previously defined transect locations to determine the start of each survey transect. We then ran out a 30 m transect in a direction perpendicular to the shoreline, though transects were stopped earlier than 30 m if divers ran into the thermocline. Divers surveyed 1 m on either side of the transect for a transect belt that is 2 m wide. 

We conducted removal surveys, which require two divers. In the removal survey, the first diver swam over the transect line marking detections for the second diver. The second diver then tried to detect animals missed by the first diver. We implemented the distance removal survey by having the primary diver swim over the transect line. Whenever the diver detected a zebra mussel or cluster of mussels, they marked the location with a survey flag then recorded the number of mussels in the cluster, the distance from the transect start to the detection (hereafter transect distance), and the perpendicular distance from the location of the detection to the transect line (hereafter detection distance). The secondary diver then looked for zebra mussels that were missed by the primary diver. Divers rotated through the primary and secondary observer roles in order to *average out* potentially innate differences between observers [@Cook1979].

### Double-observer removal surveys

This design was similar to the distance survey, though instead distance was not recorded. An important assumption in these removal surveys is that the detection probability is constant over the space surveyed, therefore ivers surveyed 0.5 m on either side of the transect for a transect belt that is 1 m wide. 


## Statistical analysis
 
* Distance sampling: the counts for each transect are denoted as $x_i$ with the total counts in the lake with $n$ transects is denoted as $X=\sum_i^n x_i$. The length of each transect is denoted as $l_i$ and the total length of $L=\sum_i^n l_i$. The encounter rate $x_i/l_i$ is used to weight the survey effort. The estimated detection probability on a transect is denoted as $\hat{P}$ and the cluster size of each detection event is denoted as $s$. The estimated density and variance in density [@Buckland2001] are given by

\begin{align}
  \hat{D} &= \frac{\displaystyle\sum_i^n  x_i}{\hat{P} A} \mathrm{E}(s) \label{eq:hatD}\\
  \mathrm{var}(\hat{D}) &= \hat{D}^2 \left(  \frac{\mathrm{var}(X)}{X^2}+ \frac{\mathrm{var}(\mathrm{E}(s))}{\mathrm{E}(s)^2} + \frac{\mathrm{var}(A\hat{P})}{A^2\hat{P}^2} \right), \label{eq:varD}
\end{align}
with the total survey area $A=\displaystyle\sum_i^n a_i$, variance in total counts $\mathrm{var}(X) = \left(L \sum_{i}^n l_i (x_i/l_i - X/L)^2 \right)/(n-1)$, and the expected value and variance in the expected cluster size (denoted as $\mathrm{E}(s)$ and $\mathrm{var}(\mathrm{E}(s))$, respectively. The variance in the detectability $\mathrm{var}(A\hat{P}$ is **XX**. Below we describe how to determine the detection probability component using distance sampling.

In distance sampling the transect distance is used to estimate how detectability changes with distance from the transect line. We modeled detection probabilities using two model subcomponents. The distance component of the detection function, describes how distance ($y$) leads to changes in the probability of detecting a zebra mussel. Based on our initial inspection of the detections we used the hazard-rate detectability model, which incorporates a shoulder in the detection function. A major assumption in the conventional distance sampling framework is that detection is perfect on the transect line, an assumption that has been shown to not hold for zebra mussel surveys [@Ferguson2018]. The mark-recapture component of the model is used to relax this assumption by using the removal surveys to estimate detectability on the transect line. In the mark-recapture model we assumed that each dive team collected data independently; thus, we used the point independence assumption described by @Borchers2006. Point independence accounts for the effects of unmodeled covariates that can induce unexpected correlations between observers. We also included the effect of cluster size as a covariate influencing detection in the mark-recapture component of the model.  We estimated detection using the mrds package [@Laake2018], which accounts for removal survey and for the effect of distance on detectability. We obtain the transect-level detectability $\hat{P}$ of detecting an zebra mussel cluster by integrating the detectability function over the transect strip-width [@Buckland2001]. 

* We obtained estimates of density from double observer sampling by using a removal survey estimator using Zippins estimator. We have also tried several other techniques including fitting with mrds (and ignoring the distance component)

We obtained estimates of density from quadrat sampling by modifying equations \ref{eq:hatD} and \ref{eq:varD} to assume that detection is perfect ($\hat{P} = 0$ and $\mathrm{var}\left(\hat{P}\right)=0$) and to account for counting individuals, not clusters ($\mathrm{E}(s)=1$ and $\mathrm{var}\left( \mathrm{E}(S) \right)=0$). 


## Time budget analysis

* In order to determine the efficiency of each method, we quantified how much time each method required to complete a transect. 
* We modeled the time to conduct a transect survey by breaking up the total survey time into two components, the time needed to setup the transect and the time needed to perform the transect survey, which accounts for swimming the transect and recording each detection. The first component, the time to setup the transect survey (hereafter referred to as the setup time), was the time required to place the transect line(s). We modeled the setup time using linear mixed-effects regression analysis with fixed-effect covariates of survey type (distance or quadrat, included as a qualitative variable), and the transect length (continuous variable). The second component of the time budget was the time needed to perform the survey on each transect (hereafter referred to as the survey time). The model structure of this component included fixed effects of transect length, an interaction between the type of survey and the detection rate (number of detections per meter of transect length) and a random slope term allowing detection time to vary with lake to account for any heterogeneity that occurs due to lake conditions. We note that this survey time can be broken up into a piece that is proportional to the number of detections, which we term the handling time, and a piece that is due to the time it takes to swim the transect that is proportional to the transect length.

## Survey strategies

* We used the fitted models to predict the setup time and search time for a transect of length 30 m under each survey type. 
* We combined the the setup time model and the search time model to predict the coefficient of variation for each survey across a range of densities under a fixed amount of time (here 10 hours) to determine whether the best survey strategy changes with density. 

# Results

## Surveys

* Due to a reproductive event on Little Birch lake we were only able to complete 4 transects, instead 15.
* Distance surveys allowed us to cover the most area of all the methods (Table \ref{tab:survSummTable}). Estimates of density along with the corresponding standard errors are give in (Figure \ref{fig:densEstFig}). Consistent with our initial searches on these lakes, we estimated that Lake Florida had the lowest density, Lake Burgan had an intermediate density, and Little Birch Lake had the highest density. 

```{r survSummTable, echo=F}

quad.est <- c(length(LF.quadrat.est$Area), sum(LF.quadrat.est$Area), sum(LF.quadrat.est$Mussels), length(LB.quadrat.est$Area), sum(LB.quadrat.est$Area), sum(LB.quadrat.est$Mussels), length(LBL.quadrat.est.subset$Area), sum(LBL.quadrat.est.subset$Area), sum(LBL.quadrat.est.subset$Mussels))

dist.est <- c(length(LF.distance.est$Area), sum(LF.distance.est$Area), sum(LF.distance.est$Detections), length(LB.distance.est$Area), sum(LB.distance.est$Area), sum(LB.distance.est$Detections), length(LBL.distance.est$Area), sum(LBL.distance.est$Area), sum(LBL.distance.est$Detections))

double.est <- c(length(LF.double.est$Area), sum(LF.double.est$Area), sum(LF.double.est$Detections), length(LB.double.est$Area), sum(LB.double.est$Area), sum(LB.double.est$Detections), length(LBL.double.est$Area), sum(LBL.double.est$Area), sum(LBL.double.est$Detections))

dataf <- data.frame(rbind(quad.est, dist.est, double.est))
dataf <- cbind(Design = c("Quadrat", "Distance", "Double"), dataf)

kable(dataf, digits=2, booktabs=T, row.names=F, col.names=c("Design", "Transects", "Area surveyed", "Detections", "Transects", "Area surveyed", "Detections", "Transects", "Area surveyed", "Detections"), caption="Summary of survey results.", align='lccccccccc') %>% add_header_above(c("", "Lake Florida"=3, "Lake Burgan"=3, "Little Birch Lake"=3), bold=F) 

```



```{r densEstFig, echo=F, fig.asp=0.4, fig.cap="Density estimates and standard errors. These double observer SE's in Little Birch  look way too low. Investigate."}
dataf <- data.frame(Lake=rep(c("Lake Florida", "Lake Burgan", "Little Birch Lake"),3), Design=c(rep("Quadrat", 3), rep("Double",3), rep("Distance", 3)), Estimate=c(LF.quadrat.est$Dhat, LB.quadrat.est$Dhat, LBL.quadrat.est.subset$Dhat, LF.double.est$Dhat, LB.double.est$Dhat, LBL.double.est$Dhat, LF.distance.est$Dhat, LB.distance.est$Dhat, LBL.distance.est$Dhat), SE=c(LF.quadrat.est$Dhat.se, LB.quadrat.est$Dhat.se, LBL.quadrat.est.subset$Dhat.se, LF.double.est$Dhat.se, LB.double.est$Dhat.se, LBL.double.est$Dhat.se, LF.distance.est$Dhat.se, LB.distance.est$Dhat.se, LBL.distance.est$Dhat.se))

#dataf <- data.frame(Lake=rep(c("Lake Florida \n (low density)", "Lake Burgan \n (medium density)", "Little Birch Lake \n (high density)"),2), Design=c(rep("Quadrat", 3), rep("Distance", 3)), Estimate=c(LF.quadrat.est$Dhat, LB.quadrat.est$Dhat, LBL.quadrat.est.subset$Dhat, LF.distance.est$Dhat, LB.distance.est$Dhat, LBL.distance.est$Dhat), SE=c(LF.quadrat.est$Dhat.se, LB.quadrat.est$Dhat.se, LBL.quadrat.est.subset$Dhat.se, LF.distance.est$Dhat.se, LB.distance.est$Dhat.se, LBL.distance.est$Dhat.se))

dataf$Lake <- relevel(dataf$Lake, "Lake Florida")
surveyTypeCols <- wes_palette("Darjeeling1")
ggplot(dataf, aes(x=Design, y=Estimate, colour=Design)) + geom_point(size=4) + facet_wrap(~Lake, ncol=3, scales="free") + geom_errorbar(aes(ymin=Estimate-SE, ymax=Estimate+SE, width=0)) + labs(x="", y="Density estimate") + theme_classic() + scale_fill_manual(values=surveyTypeCols) + scale_colour_manual(values=surveyTypeCols) + guides(fill=FALSE)

#print(p)

```

* Cluster sizes were similar between the distance survey and double observer  survey in all lakes. We found that the average cluster size in  Lake Florida all detections were of single zebra mussels, in Lake Burgan the average cluster size in the distance survey was `r round(LB.distance.est$eS, 2)` (SD=`r round(sqrt(LB.distance.est$varS), 2)`) and `r round(LB.double.est$eS, 2)` (SD=`r round(sqrt(LB.double.est$varS), 2)`) in the double observer survey. In Little Birch Lake there were larger and more variable clusters in both the distance survey, with an  average cluster size of `r round(LBL.distance.est$eS,2)` (SD=`r round(sqrt(LBL.distance.est$varS), 2)`), and in the double observer survey with an average cluster sise of `r round(LBL.double.est$eS,2)` (SD=`r round(sqrt(LBL.double.est$varS), 2)`).

```{r detectDist, echo=F, warning=F, fig.asp=0.4, fig.cap='Stacked histogram showing the total number zebra mussel detections made by the primary and secondary diver in each lake. Distance bin widths are 0.1 m.'}

distance.dat <- rbind(LB.distance.est$distance.dat, LBL.distance.est$distance.dat, LF.distance.est$distance.dat)

distance.dat$Lake     <- factor(distance.dat$'Lake name', levels = c("Lake Florida", "Lake Burgan", "Little Birch Lake"))

#remove secondary observerations that were also made by primary
rm.ind <- NULL
for(j in levels(distance.dat$Lake)) {
for(i in unique(subset(distance.dat, Lake == j)$object)) {
    ind <- which(distance.dat$Lake == j & distance.dat$object == i)
    if(all(distance.dat$detected[ind] == 1)) {
      rm.ind <- c(rm.ind, ind[which(distance.dat$observer[ind] == 2)] )
    }
  }
}
distance.dat <- distance.dat[-rm.ind,]

#remove observertations that weren't actually made
distance.dat          <- distance.dat %>% subset(detected==1) %>% mutate(Observer = factor(observer)) #%>% 

distance.dat$Observer <-  recode(distance.dat$observer, "1"="Primary", "2"="Secondary")

#ggplot(data=distance.dat, aes(x=distance, fill=Observer)) + geom_histogram(binwidth=0.1, col="black") + xlab("Distance from transect line") + ylab("Detections") + theme_bw() + scale_fill_manual(values=wes_palette("Moonrise2"))  + facet_wrap(~Lake) + theme(axis.text=element_text(size=10), axis.title=element_text(size=14), axis.line = element_line(colour = "black"), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank(), panel.background = element_blank()) + theme_classic()
  
```
* Distance surveys: amount of area surveyed and number of detections in each lake are given in Table \ref{tab:survSummTable}. The average cluster size of detections in Lake Florida was , density estimate and standard error. Estimates of density along with the corresponding standard errors are give in (Figure \ref{fig:densEstFig}). We determined that Lake Florida has the lowest density, Little Birch Lake had the highest density, and Lake Burgan had an intermediate density. 
* Our estimates of the distance detection functions indicated that the detection probabilities were similar between lakes with the estimated transect detectability in Lake Florida (reported as mean (standard error)) `r round(LF.distance.est$phat, 2)` (`r round(LF.distance.est$phat.se, 2)`), Lake Burgan `r round(LB.distance.est$phat, 2)` (`r round(LB.distance.est$phat.se, 2)`), and in Little Birch Lake `r round(LBL.distance.est$phat, 2)` (`r round(LBL.distance.est$phat.se, 2)`). 
*  We found estimates of detection using the double-observer detection survey tended to decline with increases in density, though detectability was always higher in the double-observer survey than in the distance survey  (Figure \ref{fig:detectionEst}). 
* Not clear why the distance variance is so high on Little Birch Lake for the distance survey but not the  double observer method. They should be roughly comparable. 


```{r detectFunc, echo=F, warning=F, fig.asp=1, out.width = '50%', fig.cap="Average detection probability as a function of distance estimated using distance sampling for each lake. Consider removing figure..."}

lake.cols <- wes_palette("FantasticFox1", 3)
LBL.lines <- hazardDetect.predict(LBL.distance.est$ddf)
LB.lines  <- hazardDetect.predict(LB.distance.est$ddf)
#LF.lines  <- halfnormDetect.predict(LF.distance.est$ddf)

#plot(LB.lines$x, LB.lines$y, lwd=2, xlab="Distance from transect", ylab="Probability of detection by either observer", type='l', col=lake.cols[2], ylim=c(0,1))

#lines(LF.lines$x, LF.lines$y, lwd=2, col=lake.cols[1])
#lines(LBL.lines$x, LBL.lines$y, lwd=2, col=lake.cols[3])
#legend('topright', lwd=2, col=lake.cols, legend=c("Lake Florida", "Lake Burgan", "Little Birch Lake"))
```

```{r detectionEst, echo=F, warnings=F, out.width='50%', fig.asp=0.75, messages=F, fig.cap="Numbers on the figure denote the total number of detections used to estimate detection probabilities. Bars are two standard errors. Horizontal dashed lines give the average across lakes (remove?). ", fig.align="center"}
col.vec  <- c(surveyTypeCols[1], surveyTypeCols[2])
phat.vec  <- c(LF.distance.est$phat, LF.double.est$phat, LB.distance.est$phat, LB.double.est$phat, LBL.distance.est$phat, LBL.double.est$phat)

phat.se.vec <- c(LF.distance.est$phat.se, LF.double.est$phat.se, LB.distance.est$phat.se, LB.double.est$phat.se, LBL.distance.est$phat.se, LBL.double.est$phat.se)

ss.vec      <- c(sum(LF.distance.est$Detections), sum(LF.double.est$Detections), sum(LB.distance.est$Detections), sum(LB.double.est$Detections), sum(LBL.distance.est$Detections), sum(LBL.double.est$Detections))

par(mar=c(5.1, 6.5, 4.1, 2.1))

plotCI(x=c(1,1, 2,2, 3,3) + c(-0.1, 0.1), y=phat.vec, uiw=phat.se.vec, liw=phat.se.vec, xaxt = 'n', ylab="Estimated probability of detection \nby either observer", xlab="Lake name", col=surveyTypeCols[1:2], pch=19, lwd=2, cex=2, sfrac=0, cex.lab=1.3, ylim=c(0,1.05))
#abline(h=mean(phat.vec[c(1,3,5)]), col=col.vec[1], lty=3)
#abline(h=mean(phat.vec[c(2,4,6)]), col=col.vec[2], lty=3)
axis(1, at=c(1,2,3), labels=c("Lake Florida", "Lake Burgan", "Little Birch"))
text(x=c(1,1, 2,2, 3,3) + c(-0.05, 0.05), phat.vec-0.11, labels=ss.vec, col=col.vec)

legend('bottomleft', legend=c( "Double observer", "Distance survey"), pch=19, cex=1.3, col=surveyTypeCols[1:2])


#dataf$Lake <- relevel(dataf$Lake, "Lake Florida")
#surveyTypeCols <- wes_palette("Darjeeling1")
#ggplot(dataf, aes(x=Design, y=Estimate, colour=Design)) + geom_point(size=4) + facet_wrap(~Lake, ncol=3, scales="free") + geom_errorbar(aes(ymin=Estimate-SE, ymax=Estimate+SE, width=0)) + labs(x="", y="Densit estimate") + theme_classic() + scale_fill_manual(values=surveyTypeCols) + scale_colour_manual(values=surveyTypeCols) + guides(fill=FALSE)
```

## Time budget analysis

* Our models of the setup and search time models found that transect length had positive effects on each of these components of the time budget and detection rate had a positive impact on search time (Figure \ref{fig:timeFig}). The setup time for the double and quadrat surveys was double the setup time for the distance survey. This is consistent with using two transect lines to define double and quadrat surveys and only one for the distance survey. We also found that the handling time (proportional to the slope of the search time versus detection rate) was slightly higher in double-observer surveys than in distance and quite a bit higher than in quadrat surveys. The low handling time in quadrat surveys is expected, however we did not expect the handling time in the double-observer surveys to be higher than in the distance survey, where there was the additional measurement of detection distance that was necessary. **Need to figure out why this is...** 
Finally, the time to swim the transect ($\tau_0$) did not siginifcantly differ between the three survey types. 

```{r timebudgFig, echo=F, warning=F, fig.asp=0.5, fig.cap="Empirical times for the setup and search. Consider removing figure..."}

load('TimeBudgetEst.Rdata')
#p.enc <- ggplot(time.df, aes(x=Type, y=t.enc/60, fill=Type)) +  geom_boxplot() + labs(title="Search", x="Survey type", y = "") + theme_classic() + guides(fill=FALSE) + scale_fill_manual(values=surveyTypeCols) + ylim(200/60, 30) + theme(plot.title = element_text(hjust = 0.5))

#p.set <- ggplot(time.df, aes(x=Type, y=t.set/60, fill=Type)) +  geom_boxplot() + labs(title="Setup", x="Survey type", y = "Time (minutes)") + theme_classic() + guides(fill=FALSE) + scale_fill_manual(values=surveyTypeCols) + theme(plot.title = element_text(hjust = 0.5))

#grid.arrange(p.set, p.enc, nrow = 1)                

```



```{R timeFig, echo=F, fig.cap="The impact of transect distance and detection rate on on the setup and search time. Predictinos are of the fixed effects. Bands represent 95% confidence intervals of the predicted values.", warning=F, message=F, fig.asp=0.4}
#length.pred <- subset(length.pred, group != "Double")
#rate.pred <- subset(rate.pred, group != "Double")
#length.pred$group <- droplevels(length.pred$group)

p1 <- ggplot(subset(length.pred, Type=="Setup time"), aes(x, predicted/60, group=group, colour=group)) + geom_line() + geom_ribbon(aes(ymin = conf.low/60, ymax = conf.high/60, fill=group), alpha = .1, colour=NA) + theme_classic() + labs(x="Transect distance", y="Time (minutes)", title="Setup time") + theme(legend.position="none") + scale_fill_manual(values=surveyTypeCols) + scale_colour_manual(values=surveyTypeCols) + theme(plot.title = element_text(hjust = 0.5))

p2 <- ggplot(subset(length.pred, Type=="Encounter time"), aes(x, predicted/60, group=group, colour=group)) + geom_line() + geom_ribbon(aes(ymin = conf.low/60, ymax = conf.high/60, fill=group), alpha = .1, colour=NA) + theme_classic() + labs(x="Transect distance", y="", title="Search time") + theme(legend.position="none") + scale_fill_manual(values=surveyTypeCols) + scale_colour_manual(values=surveyTypeCols) + theme(plot.title = element_text(hjust = 0.5))

p3 <- ggplot(rate.pred, aes(x, predicted/60, group=group, color=group)) + geom_line() + geom_ribbon(aes(ymin = conf.low/60, ymax = conf.high/60, fill=group), alpha = .1, colour=NA) + theme_classic() + labs(x="Detection rate", y="", title="Search time") + scale_fill_manual(values=surveyTypeCols) + scale_colour_manual(values=surveyTypeCols) + theme(plot.title = element_text(hjust = 0.5))

grid.arrange(p1, p2, p3, nrow=1, ncol=3, widths=c(8, 8, 12))

```







# Determining an optimal strategy

Here we will combine the search time model with sampling models to determine which survey method works with hypothetical changes in density in these lakes.

### Optimization criterion

The criterion to optimize is the variance of the estimated density using standard form this is:
$$
\begin{aligned}
\frac{\mathrm{var}(\hat{D})}{\hat{D}^2} &= \frac{\mathrm{var}(X|\hat{P})}{ E[X]^2} + \frac{\mathrm{var}(\hat{P}|X)}{E[\hat{P}]^2} 
\end{aligned}
$$
We need to have expression for how the variance of$X$, the total number of zebra mussels counted, and $P$, the detection probability of each zebra mussel, change with density. We demote the number of transects as $n$ so $X=\sum_{i=1}^n X_i$, where $n$ is actually a function of $X$. This dependence is because the $n$ depends on the amount of time available, which depends on how many detections have been made. This is described in more detail in the following section but it means that $n$ is a random variable and that the variance of the total counts is $\mathrm{var}(X|\hat{P})=E[n]\mathrm{var}(X_i|\hat{P}) + E[X_i|P]^2\mathrm{var}(n)$. **John do you agree with this expression (that we must account for variation in n)?** We wil assume that the transect-level counts and detection probability are related by $X_i \sim NegBin(\mu=DaP,\, \alpha)$, where $D$ is the true zebra mussel density and $a$ is the surveyed area in a transect. Below we discuss how to calculate $\mathrm{var}(n)$.

## Incorporating the time-budget into the survey design

We want to model the search behavior of a diver looking for zebra mussels. We will follow the approach of Holling and partition the survey time into the time spent searching $\tau_S$, the time spent recording each observation, $\tau_H$, also called the handling time, and an additional fixed quantity that determines the time needed to setup the transect, $\tau_0$. Then, if $X_i$ objects are detected on a transect the time-budget equation for a single transect is
\begin{align}
  \tau_0 + \tau_S + \tau_H X_i.\label{eq:tauT}
\end{align}
This equation says that the the total time spent surveying a transect depends on the number of detections made. We will assume that $\tau_H$ is fixed but that $\tau_S$ can be modified by changing the swim velocity, at the cost of influencing the detection probability. The total expected time to conduct a transect is then $E[\tau_T] = \tau_0 + \tau_S + \tau_H E[X_i]$ and $\mathrm{var}(\tau_T) = \tau_H^2 \mathrm{var}[X_i]$. The total number of transects that can be completed in the fixed amount of time, $T$ hours, is $n=\lfloor T/\tau_T \rfloor \approx T/\tau_T$ with $E[n] = \frac{T}{\tau_0 + \tau_S + \tau_H E[X_i]}$ and $\mathrm{var}[n] = \frac{T^2\tau_H^2\mathrm{var}(X_i)}{(\tau_0 + \tau_S + \tau_H E[X_i])^2}$ .

As mentioned above, there is a tradeoff in how fast the diver swims and how well they can make detections. We model the probability of detecting a single mussel, $p$, by
\begin{align}
  p &= \frac{\tau_S}{\beta + \tau_S}.
\end{align}
Here $\beta$ is a measure of the searchers inefficiency, higher values correspond to taking a longer to have the same detection probability. An alternative is to use $p= 1- e^{-\beta\tau_S}$, a model that has been used in optimal foraging theory. Both functions fit the marshmallow search data about the same (*insert image here*).

We can use the time-budget to determine the number of transects that can be completed in a given chunk of time, $\tau_T$, as $n=\lfloor \tau_T/(\tau_0 +\tau_S + \tau_H E[X_i])\rfloor \approx \tau_T/(\tau_0 + \tau_S  + \tau_H E[X_i])$.  Because the counts vary, the number of transects also varies. This quantity can be approximated with the delta method as $\mathrm{var}(n) \approx \mathrm{var}(X_i) \left( \frac{\tau_T \tau_H}{ (\tau_0 + \tau_S  + \tau_H E[X_i])^2} \right)^2 = \mathrm{var}(X_i) E(n)^4 \tau_H^2 /\tau_T^2$. **Note that in simulations this quantity shifts CV(D) up and down, but does not qualitatively affect the shape much. Thus, robably worth setting to 0 for the optimization but need a stronger argument...**

We will consider the total counts in each transect, $X_i$ as a random variable arising from the negative binomial distribution with estimated rate $\lambda = DaP$, then $\mathrm{cv}(X)^2=\frac{E(n)\mathrm{var}(X_i) + E(X_i)^2 \mathrm{var}(X_i) E(n)^4 \tau_H^2 /\tau_T^2}{E(n)^2E(X_i)^2} = \frac{\mathrm{var}(X_i)\left( 1 + E(X_i)^2 E(n)^3 \tau_H^2 /\tau_T^2\right)}{E(n) E(X_i)^2}$, where $\alpha$ is the overdispersion. We will assume that the detections can be modeled as binomial random variables with equal detection probabilities when multiple observers are used. 


## Double observer survey

Here we consider two divers on each transect, this allows us to jointly estimate density and uncertainty in detection. An alternative we won't consider here is a single searcher using conventional distance sampling. If the detection probability for each diver is $p$ **notation** (and the probability of failing to detect is $q=1-p$) then the probability of a detection by either observer is $P = 1 - q^2$. We can get $\mathrm{var}(\hat{P})$ using the delta method,  $\mathrm{var}(\hat{P}) \approx 4 q^2 \mathrm{var}(\hat{p}) = 4 (1-p)^3 p /X$, where we have used the binomial variance, $\mathrm{var}(\hat{p}) =p(1-p)/X$. 
Plugging in $X=DanP$, the coefficient of variation is then

\begin{align}
  \mathrm{cv}(\hat{P}) \approx \frac{4 (1-p)^3 p}{DanP^3}. \label{eq:cvP}
\end{align}


### Analytic approximation

We can approximate the variance of $\hat{P}$ as $\mathrm{var}(\hat{P}) \approx 2(1-\hat{P})$. This gives an approximation for the coefficient of variation of $\mathrm{cv}(\hat{P}) \approx \frac{2 (1-P)}{DanP^2}$.

```{R linApprox, echo=F, eval=F, warning=F, fig.cap="\\label{fig:linApprox} Comparison of model and the linear approximation.", fig.asp=0.55}
par(mfrow=c(1,2))

P <- seq(0, 1, length.out=1e3)
p <- 1 - sqrt(1-P)

varP <- 4*(1-p)^3*p/P
plot(P, varP, type='l', xlab="P", ylab="var(P)", lwd=2, log="xy")
lines(P, 4*(1 - P)/2, col='red', lwd=2)
legend('topright', legend=c("var(P)", "approximation"), col=c('black', 'red'), lwd=2)

cvP <- 4*(1-p)^3*p/P^3
plot(P, cvP, type='l', xlab="P", ylab="cv(P)", lwd=2, log="xy")
lines(P, 4*(1 - P)/2/P^2, col='red', lwd=2)

```

We can use this approximation to solve for the search time that minimizes $cv(D)$ analytically, but the solution to the optimal search time is a nasty polynomial. If we assume that the counts are poisson distributed, then we can get the solution of $\tau_S^*=\sqrt{\beta \tau_0}$ for a single observer design, and a fifth order polynomial for the double-observer design: $0 = -8 \beta^4 \tau_0 + 4 b^2 {\tau_S^*}^3 + 5 \beta {\tau_S^*}^4 + {\tau_S^*}^5 - 2 (2 D a \beta^2 \tau_h + \beta^3 + 3  \beta^2 \tau_0) {\tau_S^*}^2 - 4 (2 D a \beta^3 \tau_h + \beta^4 + 3 \beta^3 \tau_0) \tau_S^*$. 

Here we assume the fifth, third, and first order (and one of the second order) terms are small, this gives the biquadratic equation $0 \approx -8 \beta^4 \tau_0  + 5 \beta {\tau_S^*}^4 - 2 (\beta^3 + 3  \beta^2 \tau_0) {\tau_S^*}^2$. The solution is $\tau_S^* = \sqrt{\frac{\beta(\beta+3\tau_0) + \beta \sqrt{\beta^2 + 166\beta\tau_0 +9 \tau_0^2}}{10}} \approx  \sqrt{\frac{\beta(\beta+3\tau_0) + \beta \sqrt{166\beta\tau_0}}{10}} \approx  \sqrt{\frac{\beta(\beta+3\tau_0) + 13 \beta \sqrt{\beta\tau_0}}{10}}$, where we approximated $\beta\sqrt{\beta^2 + 166\beta\tau_0 +9 \tau_H)^2}$ as $13 \beta \sqrt{\beta\tau_0}$. In Figure \ref{fig:linApprox} we test the approximation in the distance and double-observer surveys and it does a good job. We also tried the approximation of $\tau_S^* \approx \sqrt{\frac{3\beta\tau_0}{10}}$ (dashed line) and it does pretty well, though this breaks down as searcher inefficiency gets large.

```{R, echo=F, eval=T, fig.cap="\\label{fig:linApprox} Comparison of model and the linear approximation. We assume that Dentisty is 0.1,  but the approximation works well up to D=1. At D=10 the approximation significantly underestimates the analytic solution. Dashed line is the apprximation of ....", fig.asp=0.55}
par(mfrow=c(1,2))
objquad.fun <- function(x) {-8*b^4*t0 + 4*b^2*x^3 + 5*b*x^4 + x^5 - 2*(2*D*a*b^2*th + b^3 + 3*b^2*t0)*x^2 - 4*(2*D*a*b^3*th + b^4 + 3*b^3*t0)*x}

obj.fun <- function(x) { -8*b^4*t0 + 4*b^2*x^3 + 5*b*x^4 + x^5 - 2*(2*D*a*b^2*th + b^3 + 3*b^2*t0)*x^2 - 4*(2*D*a*b^3*th + b^4 + 3*b^3*t0)*x}
x   <- seq(0, 1, length.out=100)
D   <- 0.1
a   <- 30
b   <- 0.5 #0.04
t0  <- 0.08165083 #0.1371358
th  <- 0.008586343 #0.01411156
  

###test the linear approximation

beta.vec <- 10^seq(-6, -1, length.out=100)
ts.opt <- ts.approx <- ts.approx2 <- vector('numeric', length(beta.vec))
for(i in 1:length(beta.vec)) {
  b <- beta.vec[i]
  ts.opt[i] <- optimize(obj.fun, interval=c(0, 10), tol = .Machine$double.eps)$minimum
  ts.approx[i] <- sqrt((b*(b+ 3*t0) + 13*b*sqrt(b*t0))/10)
  ts.approx2[i] <- sqrt((b*(3*t0))/10)#sqrt((b*(b+ 3*t0))/10)
}

plot(beta.vec, ts.opt, log="xy", ylab=expression(paste("Optimal search time (", tau[S], "*)")), type='l', lwd=2, main="Double-observer survey", xlab=expression(paste("Searcher inefficiency (",beta, ")")))
lines(beta.vec, ts.approx, col="brown", lwd=2)
lines(beta.vec, ts.approx2, col="brown", lwd=2, lty=2)
legend('topleft', legend=c("Solution", "Approximation", "Second approximation"), col=c('black', 'brown'), lwd=2)


###test the linear approximation
b   <- 0.04
a   <- 30*2
t0  <- 0.1371358
th  <- 0.01411156
ts.opt <- ts.approx <- ts.approx2 <- vector('numeric', length(beta.vec))
for(i in 1:length(beta.vec)) {
  b <- beta.vec[i]
  ts.opt[i] <- optimize(obj.fun, interval=c(0, 10),  tol = .Machine$double.eps)$minimum
  ts.approx[i] <- sqrt((b*(b+ 3*t0) + 13*b*sqrt(b*t0))/10)
  ts.approx2[i] <- sqrt((b*(3*t0))/10) #sqrt((b*(b+ 3*t0))/10)
}

plot(beta.vec, ts.opt, log="xy", type='l', lwd=2, main="Distance survey", ylab=expression(paste("Optimal search time (", tau[S], "*)")), xlab=expression(paste("Searcher inefficiency (",beta, ")")))
lines(beta.vec, ts.approx, col="brown", lwd=2)
lines(beta.vec, ts.approx2, col="brown", lwd=2, lty=2)
```

To approximate under the negative binomial distribution, we end up with a ninth order polynomial. We approximate bwith a similar ad hoc process as above starting with the intercept, 8th, and 4th order terms. We get the approxiation $\tau_S^* \approx frac{b}{(D a \alpha)^{1/8}}$. This approximation doesn't do a very good job... We can also use the leading order term for approximating, this gives a solution of ${\tau_S}^* \approx \left(\frac{8 b^8 \tau_0}{Da\alpha+1}\right)^{1/9}$.


```{r doubleObsApproxTest, echo=F, eval=F}

obj.fun <- function(x) {(D*a*alpha + 1)*x^9 - 8*b^8*t0 + 9*(D*a*alpha*b + b)*x^8 + 3*(11*D*a*alpha*b^2 + 10*b^2)*x^7 + (2*D^2*a^2*alpha*b^2*th + (63*alpha*b^3 - 4*b^2*th)*D*a + 48*b^3 - 6*b^2*t0)*x^6 + 3*(4*D^2*a^2*alpha*b^3*th + 11*b^4 - 12*b^3*t0 + 2*(11*alpha*b^4 - 4*b^3*th)*D*a)*x^5 + (24*D^2*a^2*alpha*b^4*th - 7*b^5 - 92*b^4*t0 + 4*(9*alpha*b^5 - 14*b^4*th)*D*a)*x^4 + 4*(4*D^2*a^2*alpha*b^5*th - 7*b^6 - 32*b^5*t0 + 2*(alpha*b^6 - 8*b^5*th)*D*a)*x^3 - 6*(6*D*a*b^6*th + 3*b^7 + 17*b^6*t0)*x^2 - 4*(2*D*a*b^7*th + b^8 + 11*b^7*t0)*x}

x   <- seq(0, 1, length.out=100)
D   <- 0.1
a   <- 30
b   <- 0.5 #0.04
t0  <- 0.08165083 #0.1371358
th  <- 0.008586343 #0.01411156
alpha <- 10

x <- 10^seq(-1.5, 1, length.out=100)
plot(x, obj.fun(x), log="xy", type='l', lwd=2, xlab="ts")
lines(x, - 8*b^8*t0 + 9*(D*a*alpha*b + b)*x^8 + 0*3*(11*D*a*alpha*b^2 + 10*b^2)*x^7, col='brown')
lines(x, -8*b^8*t0 + 9*(D*a*alpha*b)*x^8  + (24*D^2*a^2*alpha*b^4*th - 7*b^5 - 92*b^4*t0 + 4*(9*alpha*b^5 - 14*b^4*th)*D*a)*x^4, col='red')
lines(x, -8*b^8*t0 + (D*a*alpha + 1)*x^9, col='orange')


```

```{r, echo=F, fig.cap="\\label{fig:linApproxDoubleObsNB} Comparison of model and the linear approximation. We assume that Dentisty is 0.1,  but the approximation works well up to D=1. At D=10 the approximation significantly underestimates the analytic solution. Dashed line is the apprximation of ....", fig.asp=1, eval=F}
obj.fun <- function(x) {(D*a*alpha + 1)*x^9 - 8*b^8*t0 + 9*(D*a*alpha*b + b)*x^8 + 3*(11*D*a*alpha*b^2 + 10*b^2)*x^7 + (2*D^2*a^2*alpha*b^2*th + (63*alpha*b^3 - 4*b^2*th)*D*a + 48*b^3 - 6*b^2*t0)*x^6 + 3*(4*D^2*a^2*alpha*b^3*th + 11*b^4 - 12*b^3*t0 + 2*(11*alpha*b^4 - 4*b^3*th)*D*a)*x^5 + (24*D^2*a^2*alpha*b^4*th - 7*b^5 - 92*b^4*t0 + 4*(9*alpha*b^5 - 14*b^4*th)*D*a)*x^4 + 4*(4*D^2*a^2*alpha*b^5*th - 7*b^6 - 32*b^5*t0 + 2*(alpha*b^6 - 8*b^5*th)*D*a)*x^3 - 6*(6*D*a*b^6*th + 3*b^7 + 17*b^6*t0)*x^2 - 4*(2*D*a*b^7*th + b^8 + 11*b^7*t0)*x}

D   <- 0.1
a   <- 30
b   <- 0.5 #0.04
t0  <- 0.08165083 #0.1371358
th  <- 0.008586343 #0.01411156
alpha <- 1
ts.opt <- ts.approx <- ts.approx2 <- vector('numeric', length(beta.vec))
for(i in 1:length(beta.vec)) {
  b <- beta.vec[i]
  ts.opt[i] <- optimize(obj.fun, interval=c(0, 10), tol = .Machine$double.eps)$minimum
  
  b.poly <- 24*D^2*a^2*alpha*b^4*th #(24*D^2*a^2*alpha*b^4*th - 7*b^5 - 92*b^4*t0 + 4*(9*alpha*b^5 - 14*b^4*th)*D*a)
  a.poly <- 9*(D*a*alpha*b)
  c.poly <- -8*b^8*t0
  ts.approx[i] <-  ((-b.poly + sqrt(b.poly^2 - 4*a.poly*c.poly))/(2*a.poly))^(1/4)
  ts.approx2[i] <-  (8*b^8*t0/(D*a*alpha+1))^(1/9) ##((sqrt(-4*a.poly*c.poly))/(2*a.poly))^(1/4)b^(7/2)/sqrt(D*a*alpha)#
}

plot(beta.vec, ts.opt, log="xy", ylab=expression(paste("Optimal search time (", tau[S], "*)")), type='l', lwd=2, main="Double-observer survey", xlab=expression(paste("Searcher inefficiency (",beta, ")")))
lines(beta.vec, ts.approx, col="brown", lwd=2)
lines(beta.vec, ts.approx2, col="brown", lwd=2, lty=2)
legend('topleft', legend=c("Solution", "Approximation", "leading order term"), col=c('black', 'brown', 'brown'), lwd=2, lty=c(1,1,2))

```

## Distance survey

For now we are using equation \ref{eq:cvP} for the distance survey. This is appropriate when the distance parameters are precisely estimated. For the half-normal model (easier to work with than the hazard-rate function which has two parameters)  $p = \int_0^w exp(-y^2/2\sigma^2) dy$. As above we can get the variance of the estimate, $\hat{p}$, but now we need to account for variation in the scale parameter, $\sigma$. Using my favorite formula this is $var(\hat{p}) = E[var(\hat{p}|\hat{\sigma})] + var(E[\hat{p}|\hat{\sigma}])$. Using the delta method on this second piece gives $var(\int_0^w exp(-y^2/2\sigma^2) dy) \approx var(\hat{\sigma}^2) \left(\frac{d}{d\sigma ^2} \int_0^w exp(-y^2/2\sigma^2) dy \right)=var(\hat{\sigma}^2) \left(\frac{d}{d\sigma ^2} \frac{\pi}{2}\sigma \mathrm{erf}\left(\frac{w}{\sqrt{2}\sigma}\right) \right) \approx  var(\hat{\sigma}^2) \frac{w^3}{6\sigma^4}$, where the last approximation was for $w\approx 0$. Thus, if $var(\hat{\sigma}^2)$ is small or if $\sigma$ is very large then we can discard this term. We will do so from here on out, though don't have a strong justification yet...

### Single-observer distance survey

In this case $\mathrm{var}(\hat{p}|X)=\frac{\hat{p}(1-\hat{p})}{Dan\hat{p}} = \frac{1- \hat{p}}{Dan}$, where $p = \frac{\tau_S}{\beta + \tau_S}$. The coefficent of variation in density under negative binomial counts is then $\mathrm{cv}(\hat{D}) \approx  \frac{1 + \alpha D a n \hat{p}}{D a n \hat{p}} + \frac{1- \hat{p}}{D a n \hat{p}^2}$, where we have assumed that $\mathrm{var}(n)$ is small.  This gives a seventh order polynomial, we approximate the polynomial by taking the leading order term. This gives the OK approximation of ${\tau_S}^* \approx \left(\frac{\beta^5 \tau_0 \tau_T}{Da\alpha + 1}\right)^{1/7}$, an approximation that works well at moderate-levels of density and overdispersion. Even if it's not a great approximation it does provide some intuition on how the solution behaves with changes in density, overdispersion, search efficiency and time budget. The solutions are power-laws with increases in density and overdispersion lead to shorter optimal search times, while increases in setup time, total time, and searcher inefficiency lead to increases in the optimal search time (Figure \ref{fig:ApproxSingleNB}).


```{r SingleObsApproxTestNB, echo=F, eval=T, fig.asp=0.55, fig.cap="\\label{fig:ApproxSingleNB} Comparison of model and the linear approximation. We assume that Dentisty is 0.1,  but the approximation works well up to D=1. At D=10 the approximation significantly underestimates the analytic solution. Dashed line is the apprximation of ...."}

par(mfrow=c(1,2))
obj.fun <- function(x) { 2*(D*a*alpha + 1)*x^7 - 2*b^5*t0*tt + (2*D*a*alpha*(3*b + t0 + th) + 7*b + 2*t0 + 2*th)*x^6 + 3*(2*(b^2 + b*(t0 + th))*D*a*alpha + 3*b^2 + 2*b*(t0 + th))*x^5 + (2*(b^3 + b^2*(3*t0 + 2*th) + (t0*th + th^2)*b)*D*a*alpha + 5*b^3 + b^2*(6*t0 + 6*th - tt) - (t0^2 - th^2 + t0*tt + th*tt)*b)*x^4 + (2*(b^3*t0 + b^2*t0*th)*D*a*alpha + b^4 + b^3*(2*t0 + 2*th - 3*tt) - (3*t0^2 - th^2 + 5*t0*tt + 3*th*tt)*b^2)*x^3 - 3*(b^4*tt + (t0^2 + 3*t0*tt + th*tt)*b^3)*x^2 - (b^5*tt + (t0^2 + 7*t0*tt + th*tt)*b^4)*x}


beta.vec <- 10^seq(-6, 0, length.out=100)

x   <- seq(0, 1, length.out=100)
D   <- 0.1
a   <- 30
b   <- 0.5 #0.04
t0  <- 0.08165083 #0.1371358
th  <- 0.008586343 #0.01411156
alpha <- 1

x <- 10^seq(-1.5, 1, length.out=100)
#plot(x, obj.fun(x), log="xy", type='l', lwd=2, xlab=expression(paste("Search time (", tau[S], "*)")), ylab="Objective function")
#lines(x, 2*(D*a*alpha + 1)*x^7 - 2*b^5*t0*tt, col='brown')
#lines(x, - 2*b^5*t0*tt + (2*D*a*alpha*(3*b + t0 + th) + 7*b + 2*t0 + 2*th)*x^6  + (2*(b^3*t0 + b^2*t0*th)*D*a*alpha)*x^3, col='orange')

ts.opt <- ts.approx <- ts.approx2 <- vector('numeric', length(beta.vec))
for(i in 1:length(beta.vec)) {
  b <- beta.vec[i]
  ts.opt[i] <- optimize(obj.fun, interval=c(0, 10), tol = .Machine$double.eps)$minimum

  ts.approx[i] <-  (b^5*t0*tt/(D*a*alpha + 1))^(1/7)
  ts.approx2[i] <-  b*(t0*tt/(D*a*alpha + 1))^(1/7) ##((sqrt(-4*a.poly*c.poly))/(2*a.poly))^(1/4)b^(7/2)/sqrt(D*a*alpha)#
}

plot(beta.vec, ts.opt, ylab=expression(paste("Optimal search time (", tau[S], "*)")), type='l', lwd=2, main="D=0.1, alpha=1", xlab=expression(paste("Searcher inefficiency (",beta, ")")))
lines(beta.vec, ts.approx, col="brown", lwd=2)
#legend('topleft', legend=c("Solution", "Approximation"), col=c('black', 'brown', 'brown'), lwd=2, lty=c(1,1,2))
#lines(beta.vec, ts.approx2, col="brown", lwd=2, lty=2)
#legend('topleft', legend=c("Solution", "Approximation", "Second approximation"), col=c('black', 'brown', 'brown'), lwd=2, lty=c(1,1,2))


x   <- seq(0, 1, length.out=100)
D   <- 10
a   <- 30
b   <- 0.04
t0  <- 0.08165083 #0.1371358
th  <- 0.008586343 #0.01411156
alpha <- 1

x <- 10^seq(-1.5, 1, length.out=100)
#plot(x, obj.fun(x), log="xy", type='l', lwd=2, xlab=expression(paste("Search time (", tau[S], "*)")), ylab="Objective function")
#lines(x, 2*(D*a*alpha + 1)*x^7 - 2*b^5*t0*tt, col='brown')
#lines(x, -8*b^8*t0 + (D*a*alpha + 1)*x^9, col='orange')

ts.opt <- ts.approx <- ts.approx2 <- vector('numeric', length(beta.vec))
for(i in 1:length(beta.vec)) {
  b <- beta.vec[i]
  ts.opt[i] <- optimize(obj.fun, interval=c(0, 10), tol = .Machine$double.eps)$minimum

  ts.approx[i] <-  (b^5*t0*tt/(D*a*alpha + 1))^(1/7)
  ts.approx2[i] <-  b*(t0*tt/(D*a*alpha + 1))^(1/7) ##((sqrt(-4*a.poly*c.poly))/(2*a.poly))^(1/4)b^(7/2)/sqrt(D*a*alpha)#
}

plot(beta.vec, ts.opt, ylab=expression(paste("Optimal search time (", tau[S], "*)")), type='l', lwd=2, main="D=10, alpha=1", xlab=expression(paste("Searcher inefficiency (",beta, ")")))
lines(beta.vec, ts.approx, col="brown", lwd=2)
legend('topleft', legend=c("Solution", "Approximation"), col=c('black', 'brown', 'brown'), lwd=2, lty=c(1,1,2))
#lines(beta.vec, ts.approx2, col="brown", lwd=2, lty=2)
#legend('topleft', legend=c("Solution", "Approximation", "Second approximation"), col=c('black', 'brown', 'brown'), lwd=2, lty=c(1,1,2))

```

### Double-observer distance survey


```{R doubleSolnI, echo=F, fig.asp=0.4, fig.cap="\\label{fig:doubleSolnI} Numerical estimates for the optimal solution with the double observer methodology when alpha=0.1, tT=10, t0=0.1, beta=1"}
t0    <- 0.1
beta  <- 1
tT    <- 10
a     <- 1
tH    <- 0.1
tH.vec <- seq(0, 1, length.out=100)
D.vec <- 10^(seq(-3, 1, length.out=100))
alpha.vec <- seq(0,1,length.out=100)
D     <- 1

P <- function(beta, ts) {
  p <- ts/(beta + ts)

  return(p)
}


cv.quad.sample <- function(ts, D, a, tH, tT, alpha, t0, offset=0) {
  
  P.sing <- 1
  P.dub  <- 1 
  
  n <- tT/(t0 + ts + tH*D*a*P.dub)
  #print(n)
  e.X   <- D*a*P.dub
  var.X <- e.X + alpha*(e.X)^2 + offset
  
  e.N   <- tT/(t0 + ts + tH*e.X)
  var.N <- var.X*(tT*tH/(t0 + ts + tH*e.X)^2)^2
  
  cv <- (e.N*var.X + e.X^2*var.N)/(e.N*e.X)^2
  
  return(cv)
}

cv.doub.sample <- function(ts, D, a, beta, tH, tT, alpha, m, t0) {
  
  P.sing <- P(beta, ts)
  P.dub  <- 2*P.sing - P.sing^2
  
  n <- tT/(t0 + ts + tH*D*a*P.dub)

  e.X   <- D*a*P.dub
  var.X <- e.X + alpha*(e.X)^2
  
  #var.N <- var.X*(tT*tH/(t0 + ts + tH*e.X))^2
  e.N   <- tT/(t0 + ts + tH*e.X)
  var.N <- var.X*(tT*tH/(t0 + ts + tH*e.X)^2)^2
  
  cv.P <- 4*(1-P.sing)^3*P.sing/(n*e.X*P.dub^2)
  
 cv <- (e.N*var.X + e.X^2*var.N)/(e.N*e.X)^2 + cv.P
  
  return(cv)
}


```  

# Connecting to zebra mussel surveys

From the empirical study we estimated $\tau_H$, $\tau_0$, and $\tau_S$ for each survey type (Distance, double-observer, and quadrat).  I am using the average values of the these estimates across lakes to look at when it would be best to switch sampling behaviors. We don't know $\beta$, the search inefficiency but we do have esimates of a divers detection probability, $\hat{p}$, so we will determine the $\beta$ that is most consistent with this value by solving $p=\tau_S/(\beta+\tau_S)$. Right now I am using the variance formula of the double-observer formula in the distance survey (equation \ref{eq:cvP}) so will need to update that but I expect what we have is conservative. 

```{r, echo=F, warning=F}

load("DensEst.Rdata")

th.dist <-  (927.325)/30/60^2 #??? need to divide by 30 to correct for detection rate?
th.dub  <-  (927.325 + 596.723)/30/60^2 
th.quad <-  (927.325 - 746.518)/30/60^2 

ts.dist <- sum(c(461.735, 6.928, 927.325, -206.225, -45.469, 596.723, -746.518)*c(1, 30, 0, 0, 0, 0, 0))/60^2
ts.dub  <- sum(c(461.735, 6.928, 927.325, -206.225, -45.469, 596.723, -746.518)*c(1, 30, 0, 1, 0, 0, 0))/60^2
ts.quad  <- sum(c(461.735, 6.928, 927.325, -206.225, -45.469, 596.723, -746.518)*c(1, 30, 0, 0, 1, 0, 0))/60^2

t0.dist <- sum(c(258.573, 1.179, 199.746, 162.609)*c(1, 30, 0, 0))/60^2 #+  15/60
t0.dub  <- sum(c(258.573, 1.179, 199.746, 162.609)*c(1, 30, 1, 0))/60^2 #+  15/60
t0.quad <- sum(c(258.573, 1.179, 199.746, 162.609)*c(1, 30, 0, 1))/60^2 #+  15/60

distance.Phat <- mean(c(LBL.distance.est$phat, LB.distance.est$phat))#, LF.distance.est$phat))
distance.phat <- 1 - sqrt(1-distance.Phat)
dist.beta   <- ts.dist/distance.phat - ts.dist

double.Phat <- mean(c(LBL.double.est$phat, LB.double.est$phat, LF.double.est$phat))
double.phat <- 1 - sqrt(1-double.Phat)
double.beta <- ts.dub/double.phat - ts.dub

###get alpha values
library(MASS)

LB.dub.glm  <- glm.nb(LB.double.est$Detections ~ offset(log(LB.double.est$Area)))
LB.dist.glm <- glm.nb(LB.distance.est$Detections ~ offset(log(LB.distance.est$Area)))
LB.quad.glm <- glm.nb(LB.quadrat.est$Mussels ~ offset(log(LB.quadrat.est$Area)))

LB.alpha <- 1/mean(c(summary(LB.dub.glm)$theta, summary(LB.dist.glm)$theta, summary(LB.quad.glm)$theta))

LBL.dub.glm <- glm.nb(LBL.double.est$Detections ~ offset(log(LBL.double.est$Area)))
LBL.dist.glm <- glm.nb(LBL.distance.est$Detections ~ offset(log(LBL.distance.est$Area)))
LBL.quad.glm <- glm.nb(LBL.quadrat.est$Mussels ~ offset(log(LBL.quadrat.est$Area)))

LBL.alpha <- 1/mean(c(summary(LBL.dub.glm)$theta, summary(LBL.dist.glm)$theta, summary(LBL.quad.glm)$theta))

LF.dub.glm  <- glm.nb(LF.double.est$Detections ~ offset(log(LF.double.est$Area)))
LF.dist.glm <- glm.nb(LF.distance.est$Detections ~ offset(log(LF.distance.est$Area)))
LF.quad.glm <- glm.nb(LF.quadrat.est$Mussels ~ offset(log(LF.quadrat.est$Area)))

LF.alpha <- 1/mean(c(summary(LF.dub.glm)$theta, summary(LF.dist.glm)$theta, summary(LF.quad.glm)$theta))

```

We will not consider travel time here, but we previously found that travel time was only weakly related to distance in our surveys so it's probably ok to ignore it here. We are essentially treating it as a constant term in the total setup time $\tau_0$.

```{r, echo=F}
nb.dis <- glm.nb(LF.distance.est$Detections ~ 1 + offset(log(LF.distance.est$Area)))$theta
nb.dub <- glm.nb(LF.double.est$Detections ~ 1 + offset(log(LF.double.est$Area)))$theta
nb.qua <- glm.nb(LF.quadrat.est$Mussels ~ 1 + offset(log(LF.quadrat.est$Area)))$theta

alpha.vec <- mean(nb.dis, nb.dub, nb.qua)

nb.dis <- glm.nb(LB.distance.est$Detections ~ 1 + offset(log(LB.distance.est$Area*LB.distance.est$phat)))$theta
nb.dub <- glm.nb(LB.double.est$Detections ~ 1 + offset(log(LB.double.est$Area)))$theta
nb.qua <- glm.nb(LB.quadrat.est$Mussels ~ 1 + offset(log(LB.quadrat.est$Area)))$theta

alpha.vec <- c(alpha.vec, mean(nb.dis, nb.dub, nb.qua))

nb.dis <- glm.nb(LBL.distance.est$Detections ~ 1 + offset(log(LBL.distance.est$Area)))$theta
nb.dub <- glm.nb(LBL.double.est$Detections ~ 1 + offset(log(LBL.double.est$Area)))$theta
nb.qua <- glm.nb(LBL.quadrat.est$Mussels ~ 1 + offset(log(LBL.quadrat.est$Area)))$theta

alpha.fit <- 1/c(alpha.vec, mean(nb.dis, nb.dub, nb.qua))

```

Here are the parameters used in each survey type. In addition, we used the overdispersion estimate, $\alpha$, estimated in each lake averaged across the survey designs.

**Distance survey:** 


\begin{table}[]
\centering
\caption{Time budgets}
\begin{tabular}{lccccc}
\hline
Survey  & Transect area & Search time & Handling time  & Setup time  & Detection probability \\ \hline
Distance & 60  & 0.19 & 0.009 & 0.08 & 0.45 \\ 
Double   & 30  & 0.13 & 0.014 & 0.14 & 0.95 \\ 
Quadrat  & 7.5 & 0.17 & 0.002 & 0.12 & 1    \\  \hline
\end{tabular}
\end{table}

We estimated the overdisperion in each lake as Lake Florida $\alpha$ = `r round(alpha.fit[1], 2)`, Lake Burgan $\alpha$ = `r round(alpha.fit[2], 2)`, and Little Birch Lake $\alpha$ = `r round(alpha.fit[3], 2)`.


<!-- We will assume that the variance of the distance survey can be modeled using the double observer formula... for now. -->

```{r, echo=F}

cv.calc <- function(lake.name, alpha) {
  
  tT.dist <- sum(subset(time.df, Lake==lake.name & Type=="Distance")$Time)/60^2
  tT.dub  <- sum(subset(time.df, Lake==lake.name & Type=="Double")$Time)/60^2
  tT.quad <- sum(subset(time.df, Lake==lake.name & Type=="Quadrat")$Time)/60^2
  
  for(i in 1:length(D.vec)) {
   
    cv.dist.est[i]   <- cv.doub.sample(ts=ts.dist, a=60, D=D.vec[i], beta=dist.beta, tH=th.dist, tT=mean(tT.dist, tT.dub, tT.quad), alpha=alpha, m=NULL, t0=t0.dist)
    
    cv.dub.est[i]    <- cv.doub.sample(ts=ts.dub, a=30, D=D.vec[i], beta=double.beta, tH=th.dub, tT=mean(tT.dist, tT.dub, tT.quad), alpha=alpha, m=NULL, t0=t0.dub)
    cv.dub.opt.ts[i] <- optimize(cv.doub.sample, interval=c(0, tT), tol=1e-10, tT=mean(tT.dist, tT.dub, tT.quad), a=30, D=D.vec[i], beta=double.beta, alpha=alpha, tH=th.dub, t0=t0.dub)$minimum
    cv.dub.opt.cv[i]    <- cv.doub.sample(ts=cv.dub.opt.ts[i], a=30, D=D.vec[i], beta=double.beta, tH=th.dub, tT=tT, alpha=alpha, m=NULL, t0=t0.dub)
    
    cv.quad.est[i]   <- cv.quad.sample(ts=ts.quad, a=0.5^2*30/2*2, D=D.vec[i], tH=th.quad, alpha=alpha, tT=mean(tT.dist, tT.dub, tT.quad), t0=t0.quad)
    
  }

  return(cbind(cv.dist.est, cv.dub.est, cv.quad.est, cv.dub.opt.ts, cv.dub.opt.cv))
}
```

```{r cvLakes, echo=F, fig.asp=0.4, fig.cap="\\label{fig:cvLakes} The predicted coefficient of variation, across a range of densities, for overdispersion lakes corresponding to each lake and empirical estimates of the setup time, handling time, and search time."}

par(mfrow=c(1,3))

D.vec <- 10^seq(-2, 1, length.out=100)
#need  of alpha observed in study

cv.dist.est <- cv.dist.opt.ts <- cv.dist.opt <- cv.dub.opt.ts <- cv.dub.opt <- cv.dub.est <- cv.quad.est <- vector('numeric', length(D.vec))


alpha <- alpha.fit[1]
tT.dist <- sum(subset(time.df, Lake=="Lake Florida" & Type=="Distance")$Time)/60^2
tT.dub  <- sum(subset(time.df, Lake=="Lake Florida" & Type=="Double")$Time)/60^2
tT.quad <- sum(subset(time.df, Lake=="Lake Florida" & Type=="Quadrat")$Time)/60^2

for(i in 1:length(D.vec)) {

  cv.dist.est[i]   <- cv.doub.sample(ts=ts.dist, a=60, D=D.vec[i], beta=dist.beta, tH=th.dist, tT=mean(tT.dist, tT.dub, tT.quad), alpha=alpha.fit[1], m=NULL, t0=t0.dist)
  cv.dist.opt.ts[i] <- optimize(cv.doub.sample, interval=c(0, tT), tol=.Machine$double.eps, tT=mean(tT.dist, tT.dub, tT.quad), a=30, D=D.vec[i], beta=dist.beta, alpha=alpha.fit[1], tH=th.dub, t0=t0.dub)$minimum
  cv.dist.opt[i]    <- cv.doub.sample(ts=cv.dist.opt.ts[i], a=30, D=D.vec[i], beta=dist.beta, tH=th.dub, tT=tT, alpha=alpha.fit[1], m=NULL, t0=t0.dist)
  
  cv.dub.est[i]    <- cv.doub.sample(ts=ts.dub, a=30, D=D.vec[i], beta=double.beta, tH=th.dub, tT=mean(tT.dist, tT.dub, tT.quad), alpha=alpha.fit[1], m=NULL, t0=t0.dub)
  cv.dub.opt.ts[i] <- optimize(cv.doub.sample, interval=c(0, tT), tol=.Machine$double.eps, tT=mean(tT.dist, tT.dub, tT.quad), a=30, D=D.vec[i], beta=double.beta, alpha=alpha.fit[1], tH=th.dub, t0=t0.dub)$minimum
  cv.dub.opt[i]    <- cv.doub.sample(ts=cv.dub.opt.ts[i], a=30, D=D.vec[i], beta=double.beta, tH=th.dub, tT=tT, alpha=alpha.fit[1], m=NULL, t0=t0.dub)

    cv.quad.est[i]   <- cv.quad.sample(ts=ts.quad, a=0.5^2*30/2*2, D=D.vec[i], tH=th.quad, alpha=alpha.fit[1], tT=mean(tT.dist, tT.dub, tT.quad), t0=t0.quad)
  
}

cv.vec <- c(cv.dist.est, cv.dub.est, cv.quad.est)
plot(D.vec, cv.dist.est, log="xy", lwd=2, type='l', xlab="Density", ylab="Coefficient of variation", main=expression(paste("Lake Florida (", alpha, "=3.92)")), col='white', ylim=c(0.1,10), cex.lab=1.2)
rug(c(LF.quadrat.est$Dhat, LF.distance.est$Dhat, LF.double.est$Dhat), ticksize=0.1, side=1, lwd=2)
#lines(D.vec, cv.dub.est, lwd=2, col="grey")
lines(D.vec, cv.quad.est, lwd=2, col="cornflowerblue")
#lines(D.vec, cv.dist.est, lwd=2, col="brown")

#lines(D.vec, cv.dub.opt, lwd=2, col="grey", lty=2)
lines(D.vec, cv.dist.opt, lwd=2, col="grey")#, lty=2)
#legend('topright', legend=c("Double-observer", "Distance", "Quadrat"), col=c('grey', 'brown', 'black'), lwd=2, lty=c(1,1,1))
legend('topright', legend=c( "Distance", "Quadrat"), col=c('grey', 'cornflowerblue'), lwd=2, lty=c(1,1))

#legend('topright', legend=c("Distance", "Double-observer", "Quadrat"), col=c('black', 'red', 'cornflowerblue'), lwd=2)

alpha <- alpha.fit[2]
tT.dist <- sum(subset(time.df, Lake=="Lake Burgan" & Type=="Distance")$Time)/60^2
tT.dub <- sum(subset(time.df, Lake=="Lake Burgan" & Type=="Double")$Time)/60^2
tT.quad <- sum(subset(time.df, Lake=="Lake Burgan" & Type=="Quadrat")$Time)/60^2

for(i in 1:length(D.vec)) {
  cv.dist.est[i] <- cv.doub.sample(ts=ts.dist, a=60, D=D.vec[i], beta=dist.beta, tH=th.dist, tT=mean(tT.dist, tT.dub, tT.quad), alpha=alpha.fit[2], m=NULL, t0=t0.dist)
  cv.dist.opt.ts[i] <- optimize(cv.doub.sample, interval=c(0, tT), tol=.Machine$double.eps, tT=mean(tT.dist, tT.dub, tT.quad), a=30, D=D.vec[i], beta=dist.beta, alpha=alpha.fit[2], tH=th.dub, t0=t0.dub)$minimum
  cv.dist.opt[i]    <- cv.doub.sample(ts=cv.dist.opt.ts[i], a=30, D=D.vec[i], beta=dist.beta, tH=th.dub, tT=tT, alpha=alpha.fit[2], m=NULL, t0=t0.dist)
  
  cv.dub.est[i]  <- cv.doub.sample(ts=ts.dub, a=30, D=D.vec[i], beta=double.beta, tH=th.dub, tT=mean(tT.dist, tT.dub, tT.quad), alpha=alpha.fit[2], m=NULL, t0=t0.dub)
  cv.dub.opt.ts[i] <- optimize(cv.doub.sample, interval=c(0, tT), tol=1e-10, tT=mean(tT.dist, tT.dub, tT.quad), a=30, D=D.vec[i], beta=double.beta, alpha=alpha.fit[1], tH=th.dub, t0=t0.dub)$minimum
  cv.dub.opt[i]    <- cv.doub.sample(ts=cv.dub.opt.ts[i], a=30, D=D.vec[i], beta=double.beta, tH=th.dub, tT=tT, alpha=alpha.fit[1], m=NULL, t0=t0.dub)
    
  cv.quad.est[i] <- cv.quad.sample(ts=ts.quad, a=0.5^2*30/2*2, D=D.vec[i],  tH=th.quad, alpha=alpha.fit[2], tT=mean(tT.dist, tT.dub, tT.quad), t0=t0.quad)
  
}

cv.vec <- c(cv.dist.est, cv.dub.est, cv.quad.est)
plot(D.vec, cv.dist.est, log="xy", lwd=2, type='l', xlab="Density", ylab="Coefficient of variation", ylim=c(0.01,10), main=expression(paste("Lake Burgen (", alpha, "=0.30)")), col="white", cex.lab=1.2)
rug(c(LB.quadrat.est$Dhat, LB.distance.est$Dhat, LB.double.est$Dhat), ticksize=0.1, side=1, lwd=2)
#lines(D.vec, cv.dub.est, lwd=2, col="grey")
lines(D.vec, cv.quad.est, lwd=2, col="cornflowerblue")
#lines(D.vec, cv.dub.opt, lwd=2, col="orangered3", lty=2)
#lines(D.vec, cv.dist.est, lwd=2, col="grey")

#lines(D.vec, cv.dub.opt, lwd=2, col="grey", lty=2)
lines(D.vec, cv.dist.opt, lwd=2, col="grey")#, lty=2)



alpha <- alpha.fit[3]
tT.dist <- sum(subset(time.df, Lake=="Little Birch Lake" & Type=="Distance")$Time)/60^2
tT.dub <- sum(subset(time.df, Lake=="Little Birch Lake" & Type=="Double")$Time)/60^2
tT.quad <- sum(subset(time.df, Lake=="Little Birch Lake" & Type=="Quadrat")$Time)/60^2

for(i in 1:length(D.vec)) {
  
  cv.dist.est[i] <- cv.doub.sample(ts=ts.dist, a=60, D=D.vec[i], beta=dist.beta, tH=th.dist, tT=mean(tT.dist, tT.dub, tT.quad), alpha=alpha.fit[3], m=NULL, t0=t0.dist)
   cv.dist.opt.ts[i] <- optimize(cv.doub.sample, interval=c(0, tT), tol=.Machine$double.eps, tT=mean(tT.dist, tT.dub, tT.quad), a=30, D=D.vec[i], beta=dist.beta, alpha=alpha.fit[2], tH=th.dub, t0=t0.dub)$minimum
  cv.dist.opt[i]    <- cv.doub.sample(ts=cv.dist.opt.ts[i], a=30, D=D.vec[i], beta=dist.beta, tH=th.dub, tT=tT, alpha=alpha.fit[2], m=NULL, t0=t0.dist)
  
  cv.dub.est[i]  <- cv.doub.sample(ts=ts.dub, a=30, D=D.vec[i], beta=double.beta, tH=th.dub, tT=mean(tT.dist, tT.dub, tT.quad), alpha=alpha.fit[3], m=NULL, t0=t0.dub)
  cv.dub.opt.ts[i] <- optimize(cv.doub.sample, interval=c(0, tT), tol=1e-10, tT=mean(tT.dist, tT.dub, tT.quad), a=30, D=D.vec[i], beta=double.beta, alpha=alpha.fit[1], tH=th.dub, t0=t0.dub)$minimum
  cv.dub.opt[i]    <- cv.doub.sample(ts=cv.dub.opt.ts[i], a=30, D=D.vec[i], beta=double.beta, tH=th.dub, tT=tT, alpha=alpha.fit[1], m=NULL, t0=t0.dub)
    
  cv.quad.est[i] <- cv.quad.sample(ts=ts.quad, a=0.5^2*30/2*2, D=D.vec[i], tH=th.quad, alpha=alpha.fit[3], tT=mean(tT.dist, tT.dub, tT.quad), t0=t0.quad)
  #print('---')
}

cv.vec <- c(cv.dist.est, cv.dub.est, cv.quad.est)
plot(D.vec, cv.dist.est, log="xy", lwd=2, type='l', xlab="Density", ylab="Coefficient of variation", ylim=c(0.01, 10), main=expression(paste("Little Birch Lake (", alpha, "=0.90)")), col='white', cex.lab=1.2)
rug(c(LBL.quadrat.est$Dhat, LBL.distance.est$Dhat, LBL.double.est$Dhat), ticksize=0.1, side=1, lwd=2)
#lines(D.vec, cv.dub.est, lwd=2, col="grey")
lines(D.vec, cv.quad.est, lwd=2, col="cornflowerblue")
#lines(D.vec, cv.dist.est, lwd=2, col="brown")
#legend('topright', legend=c("Distance", "Double-observer", "Quadrat"), col=c('black', 'red', 'cornflowerblue'), lwd=2)

#lines(D.vec, cv.dub.opt, lwd=2, col="grey", lty=2)
lines(D.vec, cv.dist.opt, lwd=2, col="grey")#, lty=2)



```


We see that the general pattern (Figure \ref{fig:cvLakes}) is for double observer and distance sampling methods to be more efficent at low densities and quadrat surveys at high densities. This is broadly consistent with what we found empirically, though the coefficient of variation of the distance survey appears to be consistently higher than we found in the data, where we had variances in distance estimates as less than half of the quadrat surveys. Could be due to the large sample variance approximation of $\hat{P}$, or something else... but maybe not worth sweating over if the qualitative conclusions hold.


### Investigating the discrepency

One reason why the quadrat counts could predicted to perform better than it is actually performing is because of measurement error. Luckily, we had our divers survey the exact same quadrats in Little Birch lake and we plot the results below:

```{r, echo=F, fig.cap="\\label{fig:DiverCounts}Counts of diver one against diver two in the exact same quadrant. The one-to-one line is in grey."}

###examine counts of both observers
#quadrat.dat      <- read_xlsx(path="../Data/Season2/Encounters - Quadrats (Responses).xlsx", sheet=1)
quadrat.dat      <- quadrat.dat %>% subset(`Lake name` == "Little Birch Lake")

quadrat.dat <- subset(quadrat.dat, (`Transect #` == 1 | `Transect #` == 2 | `Transect #` == 3 | `Transect #` == 4 | `Transect #` == 5 | `Transect #` == 6 | `Transect #` == 7 | `Transect #` == 8 | `Transect #` == 9 | `Transect #` == 10 | `Transect #` == 11))

quad1 <- subset(quadrat.dat, `Observer name` == "Aislyn")
quad2 <- subset(quadrat.dat, `Observer name` == "Austen")


quad1 <- quad1[order(quad1$`Transect #`, quad1$`Distance along transect (m)`),]
quad2 <- quad2[order(quad2$`Transect #`, quad2$`Distance along transect (m)`),]

x <- quad1$`Number of mussels in quadrat`
y <- quad2$`Number of mussels in quadrat`

v <- prcomp(cbind(x=x, y=y))$rotation
b <- v[2,1]/v[1,1]
a <- mean(y) - mean(x)*b

par(mfrow=c(1,2))
plot(x, y, xlab="Diver 1 counts", ylab="Diver 2 counts", pch=19, col=rgb(0,0,0, 0.5), xlim=c(0,80), ylim=c(0, 80))
abline(0, 1, col='grey', lwd=1, lty=2)

resid <- y - (a*x + b)
plot(x, resid, pch=19, col=rgb(0,0,0, 0.5), xlab="Aislyn's counts", ylab="Residuals")
abline(0, 1, col='grey', lwd=1, lty=2)
abline(0, -1, col='grey', lwd=1, lty=2)
```

The residual variance in the counts (from PCA but could also just do one to one line, the result is about the same) is about 32, though this error structure is clearly wrong. Here I add that variation to the total variance in counts from the negative binomial model so that $var(X) = \mu + \alpha \mu^2 + 32$. This is completely ad-hoc but should give us an idea of the impact of measurement error.

```{r cvOffsets, echo=F, fig.asp=0.4, fig.cap="\\label{fig:CVobserr} The predicted coefficient of varation, similar to figure 5, but with an addition amount of variation in the counts."}

par(mfrow=c(1,3))

D.vec <- 10^seq(-2,1.5, length.out=100)
#need  of alpha observed in study

cv.dist.est <- cv.dub.opt.ts <- cv.dub.opt <- cv.dub.est <- cv.quad.est <- vector('numeric', length(D.vec))


alpha   <- alpha.fit[1]
tT.dist <- sum(subset(time.df, Lake=="Lake Florida" & Type=="Distance")$Time)/60^2
tT.dub  <- sum(subset(time.df, Lake=="Lake Florida" & Type=="Double")$Time)/60^2
tT.quad <- sum(subset(time.df, Lake=="Lake Florida" & Type=="Quadrat")$Time)/60^2

for(i in 1:length(D.vec)) {
  cv.dist.est[i]   <- cv.doub.sample(ts=ts.dist, a=60, D=D.vec[i], beta=dist.beta, tH=th.dist, tT=mean(tT.dist, tT.dub, tT.quad), alpha=alpha, m=NULL, t0=t0.dist)
  cv.dub.est[i]    <- cv.doub.sample(ts=ts.dub, a=30, D=D.vec[i], beta=double.beta, tH=th.dub, tT=mean(tT.dist, tT.dub, tT.quad), alpha=alpha, m=NULL, t0=t0.dub)
  cv.quad.est[i]   <- cv.quad.sample(ts=ts.quad, a=0.5^2*30/2*2, D=D.vec[i], tH=th.quad, alpha=alpha, tT=mean(tT.dist, tT.dub, tT.quad), t0=t0.quad, offset=32)
  
}

cv.vec <- c(cv.dist.est, cv.dub.est, cv.quad.est)
plot(D.vec, cv.dist.est, log="xy", lwd=2, type='l', xlab="Density", ylab="Coefficient of variation", ylim=range(cv.vec), main=expression(paste("Lake Florida (", alpha, "=3.92)")), col='white')
rug(c(LF.quadrat.est$Dhat, LF.distance.est$Dhat, LF.double.est$Dhat), ticksize=0.1, side=1, lwd=2)
lines(D.vec, cv.dub.est, lwd=2, col="grey")
lines(D.vec, cv.quad.est, lwd=2, col="black")
#lines(D.vec, cv.dub.opt, lwd=2, col="orangered3", lty=2)
lines(D.vec, cv.dist.est, lwd=2, col="brown")
legend('topright', legend=c("Double-observer", "Distance", "Quadrat"), col=c('grey', 'brown', 'black'), lwd=2, lty=c(1,1,1))

#legend('topright', legend=c("Distance", "Double-observer", "Quadrat"), col=c('black', 'red', 'cornflowerblue'), lwd=2)

alpha <- alpha.fit[2]
tT.dist <- sum(subset(time.df, Lake=="Lake Burgan" & Type=="Distance")$Time)/60^2
tT.dub <- sum(subset(time.df, Lake=="Lake Burgan" & Type=="Double")$Time)/60^2
tT.quad <- sum(subset(time.df, Lake=="Lake Burgan" & Type=="Quadrat")$Time)/60^2

for(i in 1:length(D.vec)) {
  cv.dist.est[i] <- cv.doub.sample(ts=ts.dist, a=60, D=D.vec[i], beta=dist.beta, tH=th.dist, tT=mean(tT.dist, tT.dub, tT.quad), alpha=alpha, m=NULL, t0=t0.dist)

  cv.dub.est[i]  <- cv.doub.sample(ts=ts.dub, a=30, D=D.vec[i], beta=double.beta, tH=th.dub, tT=mean(tT.dist, tT.dub, tT.quad), alpha=alpha, m=NULL, t0=t0.dub)
  cv.quad.est[i] <- cv.quad.sample(ts=ts.quad, a=0.5^2*30/2*2, D=D.vec[i],  tH=th.quad, alpha=alpha, tT=mean(tT.dist, tT.dub, tT.quad), t0=t0.quad, offset=32)
  
}

cv.vec <- c(cv.dist.est, cv.dub.est, cv.quad.est)
plot(D.vec, cv.dist.est, log="xy", lwd=2, type='l', xlab="Density", ylab="Coefficient of variation", ylim=range(cv.vec), main=expression(paste("Lake Burgen (", alpha, "=0.30)")), col="white")
rug(c(LB.quadrat.est$Dhat, LB.distance.est$Dhat, LB.double.est$Dhat), ticksize=0.1, side=1, lwd=2)
lines(D.vec, cv.dub.est, lwd=2, col="grey")
lines(D.vec, cv.quad.est, lwd=2, col="black")
lines(D.vec, cv.dist.est, lwd=2, col="brown")


alpha <- alpha.fit[3]
tT.dist <- sum(subset(time.df, Lake=="Little Birch Lake" & Type=="Distance")$Time)/60^2
tT.dub <- sum(subset(time.df, Lake=="Little Birch Lake" & Type=="Double")$Time)/60^2
tT.quad <- sum(subset(time.df, Lake=="Little Birch Lake" & Type=="Quadrat")$Time)/60^2

for(i in 1:length(D.vec)) {
  cv.dist.est[i] <- cv.doub.sample(ts=ts.dist, a=60, D=D.vec[i], beta=dist.beta, tH=th.dist, tT=mean(tT.dist, tT.dub, tT.quad), alpha=alpha, m=NULL, t0=t0.dist)
  cv.dub.est[i]  <- cv.doub.sample(ts=ts.dub, a=30, D=D.vec[i], beta=double.beta, tH=th.dub, tT=mean(tT.dist, tT.dub, tT.quad), alpha=alpha, m=NULL, t0=t0.dub)
  cv.quad.est[i] <- cv.quad.sample(ts=ts.quad, a=0.5^2*30/2*2, D=D.vec[i], tH=th.quad, alpha=alpha, tT=mean(tT.dist, tT.dub, tT.quad), t0=t0.quad, offset=32)
}

cv.vec <- c(cv.dist.est, cv.dub.est, cv.quad.est)
plot(D.vec, cv.dist.est, log="xy", lwd=2, type='l', xlab="Density", ylab="Coefficient of variation", ylim=range(cv.vec), main=expression(paste("Little Birch Lake (", alpha, "=0.90)")), col='white')
rug(c(LBL.quadrat.est$Dhat, LBL.distance.est$Dhat, LBL.double.est$Dhat), ticksize=0.1, side=1, lwd=2)
lines(D.vec, cv.dub.est, lwd=2, col="grey")
lines(D.vec, cv.quad.est, lwd=2, col="black")
lines(D.vec, cv.dist.est, lwd=2, col="brown")

```

Figure \ref{fig:CVobserr} illustrates the predicted variation in Lake Burgan when we have this additional error. 
The amount of observer error in the quadrat data has the potential to be greatly influencing our prediction. 


## Optimal versus realized surveys

Is it worth optimizing searcher behavior within a fixed design? For example we can ask divers to search fast or slow but even if the diver behaves optimally, what is the payoff?


## Optimal strategy over density

Above we say that the optimal strategy switches as a function of density and the location of this switch seemed relatively constant at about 0.5 detections per meter squared (Figure \ref{fig:cvLakes}). Lets see if that changes with the degree of overdispersion.

```{r optStrat, echo=F, fig.asp=1, out.width = '50%', fig.cap="\\label{fig:optStrat}Illustration of the optimal strategy (double observer (gray) or quadrat survey with perfect detection (black)). We use the time budget parameters from Lake  Burgen and a total time budget of 8 hours."}
library(akima)
tT <- 8
num <- 25
D.vec <- 10^seq(-2, log10(25), length.out=num)
alpha.vec <- seq(0, 5, length.out=num)


opt.strat <- dub.mat <- quad.mat <- matrix(NA, length(D.vec), length(alpha.vec))

tT.dist <- sum(subset(time.df, Lake=="Lake Burgan" & Type=="Distance")$Time)/60^2
tT.dub  <- sum(subset(time.df, Lake=="Lake Burgan" & Type=="Double")$Time)/60^2
tT.quad <- sum(subset(time.df, Lake=="Lake Burgan" & Type=="Quadrat")$Time)/60^2


for(i in 1:length(D.vec)) {
  for(j in 1:length(alpha.vec)) {
    dub.mat[i,j]    <- cv.doub.sample(ts=ts.dub, a=30, D=D.vec[i], beta=double.beta, tH=th.dub, tT=mean(tT.dist, tT.dub, tT.quad), alpha=alpha.vec[j], m=NULL, t0=t0.dub)
    quad.mat[i,j]   <- cv.quad.sample(ts=ts.quad, a=0.5^2*30/2*2, D=D.vec[i], tH=th.quad, alpha=alpha.vec[j], tT=mean(tT.dist, tT.dub, tT.quad), t0=t0.quad, offset=0)
    
    opt.strat[i,j] <- as.numeric(dub.mat[i,j] < quad.mat[i,j])
  }
}


image(D.vec, alpha.vec, opt.strat, col=c('cornflowerblue', 'grey'), xlab="Density", ylab=expression(paste("Overdispersion (", alpha, ")")), log="x")

#phi <- seq(0, 1, length.out=100)
#p <- 1 - sqrt(1 - phi)
#plot(phi, (1 - p)^3*p/phi^3)

```


At low density and at low levels of overdispersion the optimal strategy is to use double observer surveys, at higher levels the strategy switches to using quadrat surveys.

```{r optSearchtime, echo=F, fig.asp=1, out.width = '50%', fig.cap="\\label{fig:optStrat}Illustration of the optimal search time parameters from Lake  Burgen and a total time budget of 8 hours."}
library(akima)
tT <- 16
num <- 100
D.vec <- seq(0.01, 10, length.out=num)#10^seq(-2, log10(10), length.out=num+10)
alpha.vec <- seq(0, 5, length.out=num)


dist.opt.ts <- matrix(NA, length(D.vec), length(alpha.vec))


for(i in 1:length(D.vec)) {
  for(j in 1:length(alpha.vec)) {
    dist.opt.ts[i,j] <- optimize(cv.doub.sample, interval=c(0, tT), tol=1e-10, tT=tT, a=30, D=D.vec[i], beta=dist.beta, alpha=alpha.vec[j], tH=th.dist, t0=t0.dist)$objective
  }
}

levels =100
filled.contour(D.vec, alpha.vec, dist.opt.ts, main="Optimal search time", nlevels=levels, plot.axes = { contour(D.vec, alpha.vec, dist.opt.ts, nlevels = 7, drawlabels = TRUE, axes = FALSE, frame.plot = FALSE, add = TRUE, col="white"); axis(1); axis(2) }, col = colorRampPalette(brewer.pal(6, "YlGnBu"))(levels+40), xlab="Density",  ylab=expression(paste("Overdispersion (", alpha, ")")), cex.lab=1.2)


```

# Some literature